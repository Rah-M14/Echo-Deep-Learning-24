{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import librosa\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision.transforms import Compose\n",
    "# import wandb\n",
    "\n",
    "# # Initialize a new wandb run\n",
    "# # wandb.init(project='TransformerWaveNet')\n",
    "\n",
    "# # Function to load an audio file and resample\n",
    "# def load_audio(file_path, sample_rate=16000):\n",
    "#     audio, sr = librosa.load(file_path, sr=sample_rate)\n",
    "#     return audio\n",
    "\n",
    "# def apply_dtw(audio, target_length, sample_rate=16000):\n",
    "#     # Generate a reference signal of target length\n",
    "#     ref_signal = np.linspace(0, 1, target_length)\n",
    "    \n",
    "#     # Ensure both signals are 2D with shape (1, N)\n",
    "#     audio = audio.reshape(1, -1)\n",
    "#     ref_signal = ref_signal.reshape(1, -1)\n",
    "    \n",
    "#     # Compute the DTW path\n",
    "#     D, wp = librosa.sequence.dtw(X=ref_signal, Y=audio, metric='euclidean')\n",
    "    \n",
    "#     # Use the DTW path to resample the audio to the target length\n",
    "#     # Extract the path indices that map ref_signal to audio\n",
    "#     path_indices = wp[:, 1]  # Get the indices from the warping path\n",
    "#     aligned_audio = audio.flatten()[path_indices]  # Align audio according to the path\n",
    "    \n",
    "#     # If the aligned audio is longer than the target length, trim it\n",
    "#     if len(aligned_audio) > target_length:\n",
    "#         aligned_audio = aligned_audio[:target_length]\n",
    "#     elif len(aligned_audio) < target_length:\n",
    "#         # If shorter, pad it\n",
    "#         aligned_audio = np.pad(aligned_audio, (0, target_length - len(aligned_audio)), mode='constant')\n",
    "    \n",
    "#     return aligned_audio\n",
    "\n",
    "# # Function to convert audio to a spectrogram\n",
    "# def audio_to_spectrogram(audio, n_fft=2048, hop_length=512):\n",
    "#     spectrogram = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)\n",
    "#     spectrogram = librosa.amplitude_to_db(np.abs(spectrogram))\n",
    "#     return spectrogram\n",
    "\n",
    "# # Custom Dataset class\n",
    "# class AudioDataset(Dataset):\n",
    "#     def __init__(self, root_dir, target_length=16000, transform=None):\n",
    "#         self.root_dir = root_dir\n",
    "#         self.files = [os.path.join(dp, f) for dp, dn, filenames in os.walk(root_dir) for f in filenames if f.endswith('.mp3') or f.endswith('.wav')]\n",
    "#         self.target_length = target_length\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.files)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         audio_path = self.files[idx]\n",
    "#         audio = load_audio(audio_path)\n",
    "#         audio = apply_dtw(audio, self.target_length)\n",
    "#         spectrogram = audio_to_spectrogram(audio)\n",
    "\n",
    "#         if self.transform:\n",
    "#             spectrogram = self.transform(spectrogram)\n",
    "\n",
    "#         return torch.tensor(spectrogram, dtype=torch.float32)\n",
    "\n",
    "# # Transform to move data to GPU\n",
    "# class ToTensorGPU:\n",
    "#     def __call__(self, tensor):\n",
    "#         return torch.tensor(tensor).cuda()\n",
    "\n",
    "# # Compose transforms\n",
    "# transforms = Compose([\n",
    "#     ToTensorGPU()\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "# Function to load an audio file, resample it, and convert to spectrogram\n",
    "def load_and_process_audio(file_path, sample_rate=16000, n_fft=2048, hop_length=512, max_length=1025):\n",
    "    audio, _ = librosa.load(file_path, sr=sample_rate)\n",
    "    spectrogram = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)\n",
    "    spectrogram = librosa.amplitude_to_db(np.abs(spectrogram))\n",
    "    \n",
    "    # Pad or truncate the spectrogram to make sure all are the same size\n",
    "    if spectrogram.shape[1] < max_length:\n",
    "        # Pad the spectrogram\n",
    "        padding = max_length - spectrogram.shape[1]\n",
    "        spectrogram = np.pad(spectrogram, ((0, 0), (0, padding)), mode='constant')\n",
    "    else:\n",
    "        # Truncate the spectrogram\n",
    "        spectrogram = spectrogram[:, :max_length]\n",
    "    \n",
    "    return spectrogram\n",
    "\n",
    "# Custom Dataset class\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.files = [os.path.join(dp, f) for dp, dn, filenames in os.walk(root_dir) for f in filenames if f.endswith('.mp3') or f.endswith('.wav')]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.files[idx]\n",
    "        spectrogram = load_and_process_audio(audio_path)\n",
    "        if self.transform:\n",
    "            spectrogram = self.transform(spectrogram)\n",
    "        return torch.tensor(spectrogram, dtype=torch.float32, device='cuda')  # Directly create tensor on GPU\n",
    "\n",
    "# Example usage of the dataset and DataLoader\n",
    "dataset = AudioDataset(root_dir='DATA')\n",
    "loader = DataLoader(dataset, batch_size=10, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 37412, 17824, 37448, 33588) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\orc\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\orc\\Lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example of iterating over the DataLoader\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mInput batch size:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\orc\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\orc\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\orc\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\orc\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1145\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 37412, 17824, 37448, 33588) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "# Example of iterating over the DataLoader\n",
    "for batch in loader:\n",
    "    print(\"Input batch size:\", batch.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class ConditionalWaveNet(nn.Module):\n",
    "#     def __init__(self, audio_channels=1, num_channels=64, kernel_size=2, num_blocks=4, num_layers=10, num_condition_classes=10):\n",
    "#         super(ConditionalWaveNet, self).__init__()\n",
    "#         self.num_blocks = num_blocks\n",
    "#         self.num_layers = num_layers\n",
    "#         self.dilated_convs = nn.ModuleList()\n",
    "#         self.condition_convs = nn.ModuleList()\n",
    "#         self.residual_convs = nn.ModuleList()\n",
    "#         self.skip_convs = nn.ModuleList()\n",
    "        \n",
    "#         # Embedding for conditioning\n",
    "#         self.embedding = nn.Embedding(num_condition_classes, num_channels)\n",
    "        \n",
    "#         # Layers for processing the spectrogram input\n",
    "#         self.spectrogram_conv = nn.Conv1d(audio_channels, num_channels, 1)\n",
    "        \n",
    "#         # Dilated convolutions and condition convolutions\n",
    "#         for _ in range(num_blocks):\n",
    "#             for i in range(num_layers):\n",
    "#                 dilation = 2 ** i\n",
    "#                 self.dilated_convs.append(nn.Conv1d(num_channels, 2 * num_channels, kernel_size, dilation=dilation, padding=dilation))\n",
    "#                 self.condition_convs.append(nn.Conv1d(num_channels, 2 * num_channels, 1))\n",
    "#                 self.residual_convs.append(nn.Conv1d(num_channels, num_channels, 1))\n",
    "#                 self.skip_convs.append(nn.Conv1d(num_channels, num_channels, 1))\n",
    "        \n",
    "#         # Attention layer\n",
    "#         self.attention = nn.MultiheadAttention(num_channels, num_heads=8)\n",
    "        \n",
    "#         # Output layers\n",
    "#         self.final_conv1 = nn.Conv1d(num_channels, num_channels, 1)\n",
    "#         self.final_conv2 = nn.Conv1d(num_channels, audio_channels, 1)\n",
    "\n",
    "#     def forward(self, audio, spectrogram, condition):\n",
    "#         # Embedding for condition\n",
    "#         condition_embedding = self.embedding(condition)\n",
    "        \n",
    "#         # Process spectrogram\n",
    "#         spectrogram = self.spectrogram_conv(spectrogram)\n",
    "        \n",
    "#         # Combine audio and spectrogram\n",
    "#         x = audio + spectrogram\n",
    "        \n",
    "#         skip_connections = []\n",
    "        \n",
    "#         for b in range(self.num_blocks):\n",
    "#             for l in range(self.num_layers):\n",
    "#                 # Dilated convolution\n",
    "#                 dilated = self.dilated_convs[b * self.num_layers + l](x)\n",
    "#                 # Conditioned convolution\n",
    "#                 conditioned = self.condition_convs[b * self.num_layers + l](condition_embedding)\n",
    "#                 # Split for gated activation\n",
    "#                 filtered, gate = torch.split(dilated + conditioned, dilated.size(1) // 2, dim=1)\n",
    "#                 x = torch.tanh(filtered) * torch.sigmoid(gate)\n",
    "#                 # Residual and skip connections\n",
    "#                 x = self.residual_convs[b * self.num_layers + l](x)\n",
    "#                 skip = self.skip_convs[b * self.num_layers + l](x)\n",
    "#                 skip_connections.append(skip)\n",
    "        \n",
    "#         # Sum all skip connections\n",
    "#         x = torch.sum(torch.stack(skip_connections), dim=0)\n",
    "        \n",
    "#         # Apply attention\n",
    "#         x = x.permute(2, 0, 1)  # Rearrange for attention\n",
    "#         x, _ = self.attention(x, x, x)\n",
    "#         x = x.permute(1, 2, 0)  # Rearrange back\n",
    "        \n",
    "#         # Final convolutions\n",
    "#         x = F.relu(self.final_conv1(x))\n",
    "#         x = self.final_conv2(x)\n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerWaveNet(nn.Module):\n",
    "    def __init__(self, audio_channels=1, num_channels=64, kernel_size=2, num_blocks=4, num_layers=10, num_condition_classes=10, num_heads=8):\n",
    "        super(TransformerWaveNet, self).__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_layers = num_layers\n",
    "        self.dilated_convs = nn.ModuleList()\n",
    "        self.condition_convs = nn.ModuleList()\n",
    "        self.residual_convs = nn.ModuleList()\n",
    "        self.skip_convs = nn.ModuleList()\n",
    "        \n",
    "        # Embedding for conditioning\n",
    "        self.embedding = nn.Embedding(num_condition_classes, num_channels)\n",
    "        \n",
    "        # Initial convolution layer for raw audio\n",
    "        self.audio_conv = nn.Conv1d(audio_channels, num_channels, 1)\n",
    "        \n",
    "        # Initial convolution layer for spectrogram\n",
    "        self.spectrogram_conv = nn.Conv1d(audio_channels, num_channels, 1)\n",
    "        \n",
    "        # Transformer block\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=num_channels, nhead=num_heads, dim_feedforward=num_channels * 4, batch_first=True),\n",
    "            num_layers=3)\n",
    "        \n",
    "        # Dilated convolutions and condition convolutions\n",
    "        for _ in range(num_blocks):\n",
    "            for i in range(num_layers):\n",
    "                dilation = 2 ** i\n",
    "                self.dilated_convs.append(nn.Conv1d(num_channels, 2 * num_channels, kernel_size, dilation=dilation, padding=dilation))\n",
    "                self.condition_convs.append(nn.Conv1d(num_channels, 2 * num_channels, 1))\n",
    "                self.residual_convs.append(nn.Conv1d(num_channels, num_channels, 1))\n",
    "                self.skip_convs.append(nn.Conv1d(num_channels, num_channels, 1))\n",
    "        \n",
    "        # Output layers\n",
    "        self.final_conv1 = nn.Conv1d(num_channels, num_channels, 1)\n",
    "        self.final_conv2 = nn.Conv1d(num_channels, audio_channels, 1)\n",
    "\n",
    "    def forward(self, audio, spectrogram, condition):\n",
    "        # Embedding for condition\n",
    "        condition_embedding = self.embedding(condition)\n",
    "        \n",
    "        # Process audio and spectrogram\n",
    "        audio = self.audio_conv(audio)\n",
    "        spectrogram = self.spectrogram_conv(spectrogram)\n",
    "        \n",
    "        # Combine audio and spectrogram\n",
    "        x = audio + spectrogram\n",
    "        \n",
    "        # Transformer processing\n",
    "        # x = x.permute(2, 0, 1)  # Rearrange for transformer (seq_len, batch, features)\n",
    "        x = self.transformer(x)\n",
    "        # x = x.permute(1, 2, 0)  # Rearrange back (batch, features, seq_len)\n",
    "        \n",
    "        skip_connections = []\n",
    "        \n",
    "        for b in range(self.num_blocks):\n",
    "            for l in range(self.num_layers):\n",
    "                # Dilated convolution\n",
    "                dilated = self.dilated_convs[b * self.num_layers + l](x)\n",
    "                # Conditioned convolution\n",
    "                conditioned = self.condition_convs[b * self.num_layers + l](condition_embedding)\n",
    "                # Split for gated activation\n",
    "                filtered, gate = torch.split(dilated + conditioned, dilated.size(1) // 2, dim=1)\n",
    "                x = torch.tanh(filtered) * torch.sigmoid(gate)\n",
    "                # Residual and skip connections\n",
    "                x = self.residual_convs[b * self.num_layers + l](x)\n",
    "                skip = self.skip_convs[b * self.num_layers + l](x)\n",
    "                skip_connections.append(skip)\n",
    "        \n",
    "        # Sum all skip connections\n",
    "        x = torch.sum(torch.stack(skip_connections), dim=0)\n",
    "        \n",
    "        # Final convolutions\n",
    "        x = F.relu(self.final_conv1(x))\n",
    "        x = self.final_conv2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def generate(self, audio, spectrogram, condition):\n",
    "        \"\"\"\n",
    "        Generate audio using the model in an autoregressive manner.\n",
    "        Assumes the model is already trained and in eval mode.\n",
    "        \"\"\"\n",
    "        self.eval()  # Ensure the model is in evaluation mode\n",
    "        with torch.no_grad():  # No need to track gradients\n",
    "            # Assuming the inputs are already on the correct device and preprocessed\n",
    "            generated_audio = self.forward(audio, spectrogram, condition)\n",
    "\n",
    "            # Post-processing if necessary (e.g., applying a sigmoid to ensure output is in the correct range)\n",
    "            generated_audio = torch.sigmoid(generated_audio)  # Example post-processing\n",
    "\n",
    "        return generated_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\rahat/.cache\\torch\\hub\\harritaylor_torchvggish_master\n",
      "Downloading: \"https://github.com/harritaylor/torchvggish/releases/download/v0.1/vggish-10086976.pth\" to C:\\Users\\rahat/.cache\\torch\\hub\\checkpoints\\vggish-10086976.pth\n",
      "100%|██████████| 275M/275M [01:20<00:00, 3.58MB/s] \n",
      "Downloading: \"https://github.com/harritaylor/torchvggish/releases/download/v0.1/vggish_pca_params-970ea276.pth\" to C:\\Users\\rahat/.cache\\torch\\hub\\checkpoints\\vggish_pca_params-970ea276.pth\n",
      "100%|██████████| 177k/177k [00:00<00:00, 1.69MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load a pre-trained VGGish model for audio feature extraction\n",
    "vggish = torch.hub.load('harritaylor/torchvggish', 'vggish')\n",
    "\n",
    "# Define the Perceptual Loss using VGGish as the feature extractor\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.feature_extractor.eval()  # Set to evaluation mode\n",
    "\n",
    "    def forward(self, generated_audio, target_audio):\n",
    "        with torch.no_grad():\n",
    "            real_features = self.feature_extractor(target_audio)\n",
    "        generated_features = self.feature_extractor(generated_audio)\n",
    "        loss = F.l1_loss(generated_features, real_features)\n",
    "        return loss\n",
    "\n",
    "perceptual_loss = PerceptualLoss(vggish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleSpectrogramLoss(nn.Module):\n",
    "    def __init__(self, scales=[1024, 2048, 4096]):\n",
    "        super(MultiScaleSpectrogramLoss, self).__init__()\n",
    "        self.scales = scales\n",
    "\n",
    "    def forward(self, generated_audio, target_audio):\n",
    "        loss = 0\n",
    "        for scale in self.scales:\n",
    "            gen_spec = torch.stft(generated_audio, n_fft=scale, return_complex=True)\n",
    "            target_spec = torch.stft(target_audio, n_fft=scale, return_complex=True)\n",
    "            loss += F.l1_loss(gen_spec.abs(), target_spec.abs())\n",
    "        return loss / len(self.scales)\n",
    "\n",
    "spectrogram_loss = MultiScaleSpectrogramLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration, let's assume we have a simple CNN as a discriminator\n",
    "class SimpleAudioDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleAudioDiscriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(16 * 16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "    def intermediate_forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return x\n",
    "\n",
    "discriminator = SimpleAudioDiscriminator()\n",
    "\n",
    "class FeatureMatchingLoss(nn.Module):\n",
    "    def __init__(self, discriminator):\n",
    "        super(FeatureMatchingLoss, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.discriminator.eval()\n",
    "\n",
    "    def forward(self, generated_audio, target_audio):\n",
    "        with torch.no_grad():\n",
    "            real_features = self.discriminator.intermediate_forward(target_audio)\n",
    "        generated_features = self.discriminator.intermediate_forward(generated_audio)\n",
    "        loss = F.l1_loss(generated_features, real_features)\n",
    "        return loss\n",
    "\n",
    "feature_matching_loss = FeatureMatchingLoss(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a composite loss\n",
    "class CompositeLoss(nn.Module):\n",
    "    def __init__(self, perceptual_loss, spectrogram_loss, feature_matching_loss):\n",
    "        super(CompositeLoss, self).__init__()\n",
    "        self.perceptual_loss = perceptual_loss\n",
    "        self.spectrogram_loss = spectrogram_loss\n",
    "        self.feature_matching_loss = feature_matching_loss\n",
    "\n",
    "    def forward(self, generated_audio, target_audio):\n",
    "        loss = (self.perceptual_loss(generated_audio, target_audio) +\n",
    "                self.spectrogram_loss(generated_audio, target_audio) +\n",
    "                self.feature_matching_loss(generated_audio, target_audio))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, epochs, device):\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for i, (audio, spectrogram, condition) in enumerate(train_loader):\n",
    "            audio, spectrogram, condition = audio.to(device), spectrogram.to(device), condition.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(audio, spectrogram, condition)\n",
    "            loss = CompositeLoss(output, audio)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Log loss to wandb\n",
    "            wandb.log({\"train_loss\": loss.item()})\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item()}\")\n",
    "            # Save model checkpoint\n",
    "            torch.save(model.state_dict(), f'TW_Checkpoint/model_TW_{epoch}.pt')\n",
    "            \n",
    "            # Generate synthetic data and add to train_loader\n",
    "            if i % 10 == 0:  # Every 10 iterations, generate synthetic data\n",
    "                with torch.no_grad():\n",
    "                    synthetic_audio = model.generate(audio, spectrogram, condition)\n",
    "                train_loader.dataset.append((synthetic_audio, spectrogram, condition))\n",
    "        \n",
    "        epoch_loss /= len(train_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss}\")\n",
    "        wandb.log({\"epoch_loss\": epoch_loss})\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for audio, spectrogram, condition in val_loader:\n",
    "                audio, spectrogram, condition = audio.to(device), spectrogram.to(device), condition.to(device)\n",
    "                output = model(audio, spectrogram, condition)\n",
    "                val_loss += criterion(output, audio).item()\n",
    "            val_loss /= len(val_loader)\n",
    "        \n",
    "        # Log validation loss to wandb\n",
    "        wandb.log({\"val_loss\": val_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable ellipsis object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MSELoss\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Assuming AudioDataset is already defined and loaded\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m train_dataset, val_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m  \u001b[38;5;66;03m# Load your datasets\u001b[39;00m\n\u001b[0;32m      6\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      7\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable ellipsis object"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False, num_workers=0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerWaveNet().to(device)\n",
    "print(model)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "# composite_loss = CompositeLoss(perceptual_loss, spectrogram_loss, feature_matching_loss)\n",
    "composite_loss = CompositeLoss(perceptual_loss, spectrogram_loss, feature_matching_loss)\n",
    "\n",
    "# train(model, train_loader, val_loader, optimizer, composite_loss, epochs=50, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
