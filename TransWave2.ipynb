{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrah-m\u001b[0m (\u001b[33mrebot\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\SEM6\\DL\\PROJ\\BEGIN\\wandb\\run-20240510_220442-ksce128h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rebot/UltimateTransformerWaveNet/runs/ksce128h' target=\"_blank\">glowing-donkey-1</a></strong> to <a href='https://wandb.ai/rebot/UltimateTransformerWaveNet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rebot/UltimateTransformerWaveNet' target=\"_blank\">https://wandb.ai/rebot/UltimateTransformerWaveNet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rebot/UltimateTransformerWaveNet/runs/ksce128h' target=\"_blank\">https://wandb.ai/rebot/UltimateTransformerWaveNet/runs/ksce128h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/rebot/UltimateTransformerWaveNet/runs/ksce128h?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2105b2fb050>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import wandb\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "wandb.init(project='UltimateTransformerWaveNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(audio_path, sample_rate=22050, duration=5):\n",
    "    # Load audio file with librosa, automatically resampling to the given sample rate\n",
    "    audio, sr = librosa.load(audio_path, sr=sample_rate, duration=duration)\n",
    "    \n",
    "    # Calculate target number of samples\n",
    "    target_length = sample_rate * duration\n",
    "    \n",
    "    # Pad audio if it is shorter than the target length\n",
    "    if len(audio) < target_length:\n",
    "        padding = target_length - len(audio)\n",
    "        audio = np.pad(audio, (0, padding), mode='constant')\n",
    "    # Truncate audio if it is longer than the target length\n",
    "    elif len(audio) > target_length:\n",
    "        audio = audio[:target_length]\n",
    "    \n",
    "    return audio\n",
    "\n",
    "def get_spectrogram(audio, n_fft=2048, hop_length=512, max_length=130):\n",
    "    # Generate a spectrogram\n",
    "    spectrogram = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)\n",
    "    # Convert to magnitude (amplitude)\n",
    "    spectrogram = np.abs(spectrogram)\n",
    "    \n",
    "    # Pad or truncate the spectrogram to ensure all are the same length\n",
    "    if spectrogram.shape[1] < max_length:\n",
    "        padding = max_length - spectrogram.shape[1]\n",
    "        spectrogram = np.pad(spectrogram, ((0, 0), (0, padding)), mode='constant')\n",
    "    else:\n",
    "        spectrogram = spectrogram[:, :max_length]\n",
    "    \n",
    "    return spectrogram\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, root_dir, sample_rate=22050, n_fft=2048, hop_length=512, max_length=130):\n",
    "        self.root_dir = root_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.max_length = max_length\n",
    "        self.files = [os.path.join(dp, f) for dp, dn, filenames in os.walk(root_dir) for f in filenames if f.endswith('.mp3') or f.endswith('.wav')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.files[idx]\n",
    "        audio = load_audio(audio_path, self.sample_rate)\n",
    "        spectrogram = get_spectrogram(audio, self.n_fft, self.hop_length, self.max_length)\n",
    "        return audio, spectrogram\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "\n",
    "    dataset = AudioDataset(root_dir='DATA')\n",
    "    loader = DataLoader(dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.01970823, -0.00225531, -0.03690785, ...,  0.02060048,\n",
      "       -0.00064142, -0.02519251], dtype=float32), array([[5.7437736e-01, 3.6425254e-01, 1.2862755e-01, ..., 7.2768354e-01,\n",
      "        2.6252899e-01, 3.3587483e-01],\n",
      "       [5.1895320e-01, 4.5031342e-01, 1.3857873e-01, ..., 5.0968748e-01,\n",
      "        4.5548519e-01, 5.7975030e-01],\n",
      "       [3.0562350e-01, 3.7505564e-01, 2.0325445e-01, ..., 3.5334751e-01,\n",
      "        4.1140336e-01, 9.5875704e-01],\n",
      "       ...,\n",
      "       [1.5232026e-03, 7.0222467e-04, 9.3422665e-07, ..., 2.8230016e-07,\n",
      "        1.7090463e-07, 1.6775441e-07],\n",
      "       [1.5125329e-03, 6.9691899e-04, 4.7191645e-07, ..., 1.4758260e-07,\n",
      "        2.3828001e-07, 3.1422653e-07],\n",
      "       [1.5089464e-03, 6.9499249e-04, 5.7717961e-07, ..., 3.2488643e-07,\n",
      "        2.2508639e-08, 2.8777606e-07]], dtype=float32))\n",
      "[ 0.01970823 -0.00225531 -0.03690785 ...  0.02060048 -0.00064142\n",
      " -0.02519251]\n",
      "(110250,)\n",
      "[[5.7437736e-01 3.6425254e-01 1.2862755e-01 ... 7.2768354e-01\n",
      "  2.6252899e-01 3.3587483e-01]\n",
      " [5.1895320e-01 4.5031342e-01 1.3857873e-01 ... 5.0968748e-01\n",
      "  4.5548519e-01 5.7975030e-01]\n",
      " [3.0562350e-01 3.7505564e-01 2.0325445e-01 ... 3.5334751e-01\n",
      "  4.1140336e-01 9.5875704e-01]\n",
      " ...\n",
      " [1.5232026e-03 7.0222467e-04 9.3422665e-07 ... 2.8230016e-07\n",
      "  1.7090463e-07 1.6775441e-07]\n",
      " [1.5125329e-03 6.9691899e-04 4.7191645e-07 ... 1.4758260e-07\n",
      "  2.3828001e-07 3.1422653e-07]\n",
      " [1.5089464e-03 6.9499249e-04 5.7717961e-07 ... 3.2488643e-07\n",
      "  2.2508639e-08 2.8777606e-07]]\n",
      "(1025, 130)\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])\n",
    "print((dataset[0][0]))\n",
    "print(dataset[0][0].shape)\n",
    "\n",
    "    \n",
    "print(dataset[0][1])\n",
    "print(dataset[0][1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(total_size * train_ratio)\n",
    "    val_size = int(total_size * val_ratio)\n",
    "    test_size = total_size - train_size - val_size  # Ensure all data is used\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "data_folder_path = 'DATA'\n",
    "# dataset = AudioDataset(root_dir=data_folder_path)\n",
    "\n",
    "# Assuming 'dataset' is an instance of AudioDataset\n",
    "train_dataset, val_dataset, test_dataset = split_dataset(dataset)\n",
    "\n",
    "# Create DataLoaders for each dataset split\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in dataset: 407\n",
      "Single batch loaded successfully: [tensor([[ 3.1908e-03, -3.7624e-03,  5.7128e-03,  ..., -6.3606e-02,\n",
      "         -3.1002e-02, -5.7576e-03],\n",
      "        [-4.2829e-02,  3.0695e-02,  3.1982e-02,  ...,  3.9355e-01,\n",
      "         -1.3681e-01,  6.6621e-02],\n",
      "        [ 5.0504e-06, -1.9910e-05,  7.9151e-06,  ..., -6.6205e-03,\n",
      "         -3.3100e-02, -6.4018e-02],\n",
      "        ...,\n",
      "        [ 5.0701e-02,  7.9188e-02,  7.8954e-02,  ...,  3.0429e-02,\n",
      "          4.0938e-02,  2.9423e-02],\n",
      "        [ 6.7924e-03,  1.0553e-02,  1.5429e-02,  ..., -3.0689e-01,\n",
      "         -3.4870e-01, -2.8935e-01],\n",
      "        [-6.9137e-02, -1.8144e-01, -2.5575e-01,  ..., -6.5567e-03,\n",
      "         -7.3422e-02, -1.0121e-01]]), tensor([[[3.9674e+01, 6.9683e+01, 6.8097e+01,  ..., 5.2310e-01,\n",
      "          4.2656e-02, 4.0288e-01],\n",
      "         [3.2716e+01, 4.3127e+01, 3.4205e+01,  ..., 1.3027e+00,\n",
      "          1.3306e+00, 1.0972e+00],\n",
      "         [1.8309e+01, 9.0223e+00, 1.2512e+00,  ..., 1.5728e+00,\n",
      "          1.4240e+00, 8.3851e-01],\n",
      "         ...,\n",
      "         [1.3377e-02, 6.6582e-03, 1.9229e-06,  ..., 7.0446e-07,\n",
      "          9.2191e-08, 4.4899e-07],\n",
      "         [1.3366e-02, 6.6532e-03, 1.8807e-06,  ..., 2.4108e-07,\n",
      "          6.7531e-07, 4.9746e-07],\n",
      "         [1.3362e-02, 6.6519e-03, 1.2317e-06,  ..., 5.5027e-07,\n",
      "          8.7410e-08, 3.9076e-08]],\n",
      "\n",
      "        [[1.4703e+01, 6.2164e+00, 4.1107e+00,  ..., 2.0230e-02,\n",
      "          2.4018e+00, 2.0957e-01],\n",
      "         [1.5319e+01, 7.7522e+00, 4.2650e+00,  ..., 3.1201e+00,\n",
      "          2.7435e+00, 2.6631e+00],\n",
      "         [1.6633e+01, 7.6748e+00, 3.3616e+00,  ..., 1.3513e+00,\n",
      "          2.1843e+00, 6.2860e-01],\n",
      "         ...,\n",
      "         [6.4270e-03, 3.4820e-03, 3.2625e-06,  ..., 5.4952e-05,\n",
      "          4.3804e-05, 3.4122e-05],\n",
      "         [6.3174e-03, 3.4379e-03, 2.0781e-06,  ..., 4.3750e-05,\n",
      "          4.1003e-05, 4.6781e-05],\n",
      "         [6.2885e-03, 3.4213e-03, 2.3484e-06,  ..., 5.2022e-05,\n",
      "          3.7441e-05, 4.5308e-05]],\n",
      "\n",
      "        [[1.0429e-04, 1.5481e-04, 9.5706e-05,  ..., 1.1829e-01,\n",
      "          8.6523e-02, 1.8991e-02],\n",
      "         [9.4254e-05, 4.7585e-05, 1.4065e-04,  ..., 3.1462e-01,\n",
      "          1.6225e-01, 2.7406e-01],\n",
      "         [5.4786e-05, 9.0937e-05, 1.9598e-04,  ..., 2.2561e-01,\n",
      "          6.6215e-02, 3.4251e-01],\n",
      "         ...,\n",
      "         [5.5988e-06, 2.7277e-06, 5.0553e-10,  ..., 1.8708e-07,\n",
      "          5.4418e-08, 7.3688e-08],\n",
      "         [5.5971e-06, 2.7303e-06, 2.6836e-09,  ..., 1.4364e-07,\n",
      "          5.2237e-08, 4.0452e-08],\n",
      "         [5.5959e-06, 2.7311e-06, 1.9548e-09,  ..., 1.2853e-07,\n",
      "          5.5647e-09, 1.4257e-08]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.5388e+00, 2.8530e-01, 5.5909e-01,  ..., 1.3228e+00,\n",
      "          1.1986e+00, 1.0428e+00],\n",
      "         [1.8488e+00, 1.2702e+00, 4.0526e-01,  ..., 4.4122e-01,\n",
      "          7.7503e-01, 6.9994e-01],\n",
      "         [2.2258e+00, 1.5172e+00, 1.0838e-01,  ..., 6.5266e-01,\n",
      "          6.7488e-01, 3.8491e-01],\n",
      "         ...,\n",
      "         [2.3034e-03, 1.2238e-03, 5.3613e-07,  ..., 3.4296e-08,\n",
      "          4.1093e-07, 2.4056e-07],\n",
      "         [2.2832e-03, 1.2155e-03, 1.5504e-06,  ..., 4.9501e-07,\n",
      "          6.5319e-07, 3.3612e-07],\n",
      "         [2.2767e-03, 1.2144e-03, 4.0182e-07,  ..., 1.4609e-07,\n",
      "          6.8380e-07, 3.7665e-07]],\n",
      "\n",
      "        [[3.2806e+00, 1.3119e+00, 1.2894e+00,  ..., 1.0099e-01,\n",
      "          4.3238e-01, 1.6896e-01],\n",
      "         [2.9513e+00, 1.7355e+00, 1.8665e+00,  ..., 4.1524e-01,\n",
      "          4.4682e-01, 3.4167e-01],\n",
      "         [3.4196e+00, 1.5410e+00, 2.1667e+00,  ..., 1.2271e-01,\n",
      "          4.3095e-01, 2.9204e-01],\n",
      "         ...,\n",
      "         [4.5131e-03, 2.2534e-03, 6.5823e-06,  ..., 5.3347e-06,\n",
      "          1.8558e-05, 8.8588e-06],\n",
      "         [4.5095e-03, 2.2569e-03, 3.9090e-06,  ..., 1.9279e-06,\n",
      "          9.1172e-06, 2.1426e-06],\n",
      "         [4.5075e-03, 2.2620e-03, 4.1511e-06,  ..., 1.5918e-06,\n",
      "          1.0572e-05, 1.9390e-06]],\n",
      "\n",
      "        [[7.9588e+00, 4.6126e+00, 2.0715e+00,  ..., 4.7845e-01,\n",
      "          6.7308e-01, 2.0460e+00],\n",
      "         [7.1025e+00, 1.8638e+00, 4.6759e+00,  ..., 4.5517e-01,\n",
      "          1.1415e+00, 7.2804e-01],\n",
      "         [1.6109e+01, 1.6698e+01, 1.7670e+01,  ..., 4.8705e-01,\n",
      "          4.1844e-01, 7.9188e-01],\n",
      "         ...,\n",
      "         [5.5204e-03, 2.8408e-03, 2.3263e-06,  ..., 5.8561e-07,\n",
      "          3.9830e-07, 9.4732e-07],\n",
      "         [5.4758e-03, 2.8263e-03, 4.6496e-06,  ..., 2.0620e-07,\n",
      "          2.5919e-07, 5.5989e-07],\n",
      "         [5.4996e-03, 2.8209e-03, 2.6620e-06,  ..., 3.9782e-07,\n",
      "          3.9811e-07, 6.2013e-08]]])]\n"
     ]
    }
   ],
   "source": [
    "# Check if the dataset is correctly set up\n",
    "print(\"Number of samples in dataset:\", len(train_dataset))\n",
    "\n",
    "# Create a DataLoader instance (make sure parameters like batch_size are set correctly)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Try to fetch a single batch to see if it works\n",
    "try:\n",
    "    data = next(iter(train_loader))\n",
    "    print(\"Single batch loaded successfully:\", data)\n",
    "except Exception as e:\n",
    "    print(\"Failed to load a batch:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 407\n",
      "Validation set size: 87\n",
      "Test set size: 88\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class TransformerWaveNet(nn.Module):\n",
    "#     def __init__(self, audio_channels=1, num_channels=64, kernel_size=2, num_blocks=4, num_layers=10, num_heads=8):\n",
    "#         super(TransformerWaveNet, self).__init__()\n",
    "#         self.num_blocks = num_blocks\n",
    "#         self.num_layers = num_layers\n",
    "#         self.dilated_convs = nn.ModuleList()\n",
    "#         self.condition_convs = nn.ModuleList()\n",
    "#         self.residual_convs = nn.ModuleList()\n",
    "#         self.skip_convs = nn.ModuleList()\n",
    "\n",
    "#         # Initial convolution layer for raw audio\n",
    "#         self.audio_conv = nn.Conv1d(audio_channels, num_channels, 1)\n",
    "#         # self.audio_conv = nn.Conv1d(10, out_channels, kernel_size)\n",
    "\n",
    "#         # Initial convolution layer for spectrogram\n",
    "#         self.spectrogram_conv = nn.Conv1d(audio_channels, num_channels, 1)\n",
    "\n",
    "#         # Transformer block\n",
    "#         self.transformer = nn.TransformerEncoder(\n",
    "#             nn.TransformerEncoderLayer(d_model=num_channels, nhead=num_heads, dim_feedforward=num_channels * 4, batch_first=True),\n",
    "#             num_layers=3)\n",
    "\n",
    "#         # Dilated convolutions and condition convolutions\n",
    "#         for _ in range(num_blocks):\n",
    "#             for i in range(num_layers):\n",
    "#                 dilation = 2 ** i\n",
    "#                 self.dilated_convs.append(nn.Conv1d(num_channels, 2 * num_channels, kernel_size, dilation=dilation, padding=dilation))\n",
    "#                 self.condition_convs.append(nn.Conv1d(num_channels, 2 * num_channels, 1))\n",
    "#                 self.residual_convs.append(nn.Conv1d(num_channels, num_channels, 1))\n",
    "#                 self.skip_convs.append(nn.Conv1d(num_channels, num_channels, 1))\n",
    "\n",
    "#         # Output layers\n",
    "#         self.final_conv1 = nn.Conv1d(num_channels, num_channels, 1)\n",
    "#         self.final_conv2 = nn.Conv1d(num_channels, audio_channels, 1)\n",
    "\n",
    "#     def forward(self, audio, spectrogram):\n",
    "#         # Process audio and spectrogram\n",
    "#         audio = self.audio_conv(audio)\n",
    "#         spectrogram = self.spectrogram_conv(spectrogram)\n",
    "\n",
    "#         # Combine audio and spectrogram\n",
    "#         x = audio + spectrogram\n",
    "\n",
    "#         # Transformer processing\n",
    "#         x = self.transformer(x)\n",
    "        \n",
    "#         skip_connections = []\n",
    "\n",
    "#         for b in range(self.num_blocks):\n",
    "#             for l in range(self.num_layers):\n",
    "#                 # Dilated convolution\n",
    "#                 dilated = self.dilated_convs[b * self.num_layers + l](x)\n",
    "#                 # Split for gated activation\n",
    "#                 filtered, gate = torch.split(dilated, dilated.size(1) // 2, dim=1)\n",
    "#                 x = torch.tanh(filtered) * torch.sigmoid(gate)\n",
    "#                 # Residual and skip connections\n",
    "#                 x = self.residual_convs[b * self.num_layers + l](x)\n",
    "#                 skip = self.skip_convs[b * self.num_layers + l](x)\n",
    "#                 skip_connections.append(skip)\n",
    "\n",
    "#         # Sum all skip connections\n",
    "#         x = torch.sum(torch.stack(skip_connections), dim=0)\n",
    "\n",
    "#         # Final convolutions\n",
    "#         x = F.relu(self.final_conv1(x))\n",
    "#         x = self.final_conv2(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "#     def generate(self, audio, spectrogram):\n",
    "#         \"\"\"\n",
    "#         Generate audio using the model in an autoregressive manner.\n",
    "#         Assumes the model is already trained and in eval mode.\n",
    "#         \"\"\"\n",
    "#         self.eval()  # Ensure the model is in evaluation mode\n",
    "#         with torch.no_grad():  # No need to track gradients\n",
    "#             # Assuming the inputs are already on the correct device and preprocessed\n",
    "#             generated_audio = self.forward(audio, spectrogram)\n",
    "\n",
    "#             # Post-processing if necessary (e.g., applying a sigmoid to ensure output is in the correct range)\n",
    "#             generated_audio = torch.sigmoid(generated_audio)  # Example post-processing\n",
    "\n",
    "#         return generated_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class TransformerWaveNet(nn.Module):\n",
    "#     def __init__(self, audio_channels=1, spectrogram_channels=1, num_channels=64, kernel_size=2, num_blocks=4, num_layers=10, num_heads=8):\n",
    "#         super(TransformerWaveNet, self).__init__()\n",
    "#         self.audio_conv = nn.Conv1d(audio_channels, num_channels, kernel_size=1)\n",
    "#         self.spectrogram_conv = nn.Conv1d(spectrogram_channels, num_channels, kernel_size=1)\n",
    "\n",
    "#         # Multi-head attention for combining audio and spectrogram features\n",
    "#         self.feature_attention = nn.MultiheadAttention(embed_dim=num_channels, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "#         # Dilated convolutions with residual connections\n",
    "#         self.audio_dilated_convs = nn.ModuleList()\n",
    "#         self.spectrogram_dilated_convs = nn.ModuleList()\n",
    "#         for i in range(num_layers):\n",
    "#             dilation = 2 ** i\n",
    "#             self.audio_dilated_convs.append(nn.Conv1d(num_channels, num_channels, kernel_size, dilation=dilation, padding=dilation, groups=num_channels//16))\n",
    "#             self.spectrogram_dilated_convs.append(nn.Conv1d(num_channels, num_channels, kernel_size, dilation=dilation, padding=dilation, groups=num_channels//16))\n",
    "\n",
    "#         # Transformer block\n",
    "#         self.transformer = nn.TransformerEncoder(\n",
    "#             nn.TransformerEncoderLayer(d_model=num_channels, nhead=num_heads, dim_feedforward=num_channels * 4, batch_first=True),\n",
    "#             num_layers=3)\n",
    "\n",
    "#         # Final output layers\n",
    "#         self.final_conv1 = nn.Conv1d(num_channels, num_channels, 1)\n",
    "#         self.final_conv2 = nn.Conv1d(num_channels, audio_channels, 1)\n",
    "\n",
    "#     def forward(self, audio, spectrogram):\n",
    "#         audio = F.relu(self.audio_conv(audio))\n",
    "#         spectrogram = F.relu(self.spectrogram_conv(spectrogram))\n",
    "\n",
    "#         # Process through dilated convolutions with residual connections\n",
    "#         for conv in self.audio_dilated_convs:\n",
    "#             audio = F.relu(conv(audio)) + audio\n",
    "#         for conv in self.spectrogram_dilated_convs:\n",
    "#             spectrogram = F.relu(conv(spectrogram)) + spectrogram\n",
    "\n",
    "#         # Combine using multi-head attention\n",
    "#         combined, _ = self.feature_attention(audio.transpose(1, 2), spectrogram.transpose(1, 2), spectrogram.transpose(1, 2))\n",
    "#         combined = combined.transpose(1, 2)\n",
    "\n",
    "#         # Transformer processing\n",
    "#         combined = self.transformer(combined.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "#         # Final processing\n",
    "#         x = F.relu(self.final_conv1(combined))\n",
    "#         x = self.final_conv2(x)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class UltimateTransformerWaveNet(nn.Module):\n",
    "#     def __init__(self, audio_channels=1, spectrogram_channels=1, num_channels=64, kernel_size=2, num_blocks=4, num_layers=10, num_heads=8):\n",
    "#         super(UltimateTransformerWaveNet, self).__init__()\n",
    "#         self.audio_conv = nn.Conv1d(audio_channels, num_channels, kernel_size=1)\n",
    "#         self.spectrogram_conv = nn.Conv1d(spectrogram_channels, num_channels, kernel_size=1)\n",
    "#         # Dilated convolutions with residual and skip connections for both streams\n",
    "#         self.audio_dilated_convs = nn.ModuleList()\n",
    "#         self.spectrogram_dilated_convs = nn.ModuleList()\n",
    "#         self.audio_res_convs = nn.ModuleList()  # Residual connections for audio\n",
    "#         self.spectrogram_res_convs = nn.ModuleList()  # Residual connections for spectrogram\n",
    "#         self.audio_skip_convs = nn.ModuleList()\n",
    "#         self.spectrogram_skip_convs = nn.ModuleList()\n",
    "        \n",
    "#         for i in range(num_layers):\n",
    "#             dilation = 2 ** i\n",
    "#             self.audio_dilated_convs.append(nn.Conv1d(num_channels, num_channels, kernel_size, dilation=dilation, padding=dilation, groups=num_channels//16))\n",
    "#             self.spectrogram_dilated_convs.append(nn.Conv1d(num_channels, num_channels, kernel_size, dilation=dilation, padding=dilation, groups=num_channels//16))\n",
    "#             self.audio_res_convs.append(nn.Conv1d(num_channels, num_channels, 1))  # Residual layer for audio\n",
    "#             self.spectrogram_res_convs.append(nn.Conv1d(num_channels, num_channels, 1))  # Residual layer for spectrogram\n",
    "#             self.audio_skip_convs.append(nn.Conv1d(num_channels, num_channels, 1))\n",
    "#             self.spectrogram_skip_convs.append(nn.Conv1d(num_channels, num_channels, 1))\n",
    "\n",
    "#         # Multi-head attention for combining features\n",
    "#         self.feature_attention = nn.MultiheadAttention(embed_dim=num_channels, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "#         # Transformer block with residual connection\n",
    "#         self.transformer = nn.TransformerEncoder(\n",
    "#             nn.TransformerEncoderLayer(d_model=num_channels, nhead=num_heads, dim_feedforward=num_channels * 4, batch_first=True),\n",
    "#             num_layers=3)\n",
    "\n",
    "#         # Output layers\n",
    "#         self.final_conv1 = nn.Conv1d(num_channels, num_channels, 1)\n",
    "#         self.final_conv2 = nn.Conv1d(num_channels, audio_channels, 1)\n",
    "\n",
    "#         # Additional residual connection across the network\n",
    "#         self.residual_conv = nn.Conv1d(num_channels, num_channels, 1)\n",
    "        \n",
    "#     def forward(self, audio, spectrogram):\n",
    "#         # Initial convolution processing\n",
    "#         audio = F.relu(self.audio_conv(audio))\n",
    "#         spectrogram = F.relu(self.spectrogram_conv(spectrogram))\n",
    "\n",
    "#         # Initialize skip connections accumulators\n",
    "#         audio_skip = 0\n",
    "#         spectrogram_skip = 0\n",
    "\n",
    "#         # Process through dilated convolutions with residual and skip connections\n",
    "#         for i in range(len(self.audio_dilated_convs)):\n",
    "#             # Apply dilated convolution\n",
    "#             audio_dilated = self.audio_dilated_convs[i](audio)\n",
    "#             spectrogram_dilated = self.spectrogram_dilated_convs[i](spectrogram)\n",
    "\n",
    "#             # Apply activation function and add residual connection\n",
    "#             audio = F.relu(audio_dilated) + self.audio_res_convs[i](audio)\n",
    "#             spectrogram = F.relu(spectrogram_dilated) + self.spectrogram_res_convs[i](spectrogram)\n",
    "\n",
    "#             # Update skip connections\n",
    "#             audio_skip += self.audio_skip_convs[i](audio)\n",
    "#             spectrogram_skip += self.spectrogram_skip_convs[i](spectrogram)\n",
    "\n",
    "#         # Combine audio and spectrogram features using multi-head attention\n",
    "#         combined, _ = self.feature_attention(audio_skip.transpose(1, 2), spectrogram_skip.transpose(1, 2), spectrogram_skip.transpose(1, 2))\n",
    "#         combined = combined.transpose(1, 2)\n",
    "\n",
    "#         # Transformer processing with additional residual connection\n",
    "#         combined_transformed = self.transformer(combined.transpose(1, 2)).transpose(1, 2)\n",
    "#         combined = combined_transformed + self.residual_conv(combined)\n",
    "\n",
    "#         # Final processing through convolution layers\n",
    "#         x = F.relu(self.final_conv1(combined))\n",
    "#         x = self.final_conv2(x)\n",
    "\n",
    "#         return x\n",
    "    \n",
    "#     def generate_audio(self, audio, spectrogram, sample_no, device='cpu'):\n",
    "#         \"\"\"\n",
    "#         Generate audio using the model and save it to a directory.\n",
    "\n",
    "#         Args:\n",
    "#         audio (torch.Tensor): The input audio tensor.\n",
    "#         spectrogram (torch.Tensor): The input spectrogram tensor.\n",
    "#         sample_no (int): The sample number to append to the filename.\n",
    "#         device (str): The device to perform computation on.\n",
    "#         \"\"\"\n",
    "#         # Ensure the model is in evaluation mode\n",
    "#         self.eval()\n",
    "#         # Move inputs to the correct device\n",
    "#         audio = audio.to(device)\n",
    "#         spectrogram = spectrogram.to(device)\n",
    "#         # Generate audio using the forward method\n",
    "#         with torch.no_grad():\n",
    "#             generated_audio = self.forward(audio, spectrogram)\n",
    "#         # Ensure the output directory exists\n",
    "#         output_dir = 'gen_music'\n",
    "#         os.makedirs(output_dir, exist_ok=True)\n",
    "#         # Save the generated audio to a file\n",
    "#         output_path = os.path.join(output_dir, f'ultranwav_{sample_no}.wav')\n",
    "#         torch.save(generated_audio, output_path)\n",
    "#         print(f\"Generated audio saved to {output_path}\")\n",
    "#         # Optionally, return the path or the audio tensor for further use\n",
    "#         return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class UltimateTransformerWaveNet(nn.Module):\n",
    "#     def __init__(self, audio_channels=1, spectrogram_channels=1025, num_channels=64, kernel_size=2, num_blocks=4, num_layers=10, num_heads=8):\n",
    "#         super(UltimateTransformerWaveNet, self).__init__()\n",
    "#         self.audio_conv = nn.Conv1d(audio_channels, num_channels, kernel_size=1)\n",
    "#         self.spectrogram_conv = nn.Conv1d(spectrogram_channels, num_channels, kernel_size=1)\n",
    "\n",
    "#         # Dilated convolutions with residual and skip connections for both streams\n",
    "#         self.audio_dilated_convs = nn.ModuleList()\n",
    "#         self.spectrogram_dilated_convs = nn.ModuleList()\n",
    "#         self.audio_skip_convs = nn.ModuleList()\n",
    "#         self.spectrogram_skip_convs = nn.ModuleList()\n",
    "#         for i in range(num_layers):\n",
    "#             dilation = 2 ** i\n",
    "#             self.audio_dilated_convs.append(nn.Conv1d(num_channels, num_channels, kernel_size, dilation=dilation, padding=dilation, groups=num_channels//16))\n",
    "#             self.spectrogram_dilated_convs.append(nn.Conv1d(num_channels, num_channels, kernel_size, dilation=dilation, padding=dilation, groups=num_channels//16))\n",
    "#             self.audio_skip_convs.append(nn.Conv1d(num_channels, num_channels, 1))\n",
    "#             self.spectrogram_skip_convs.append(nn.Conv1d(num_channels, num_channels, 1))\n",
    "\n",
    "#         # Multi-head attention for combining features\n",
    "#         self.feature_attention = nn.MultiheadAttention(embed_dim=num_channels, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "#         # Transformer block with residual connection\n",
    "#         self.transformer = nn.TransformerEncoder(\n",
    "#             nn.TransformerEncoderLayer(d_model=num_channels, nhead=num_heads, dim_feedforward=num_channels * 4, batch_first=True),\n",
    "#             num_layers=3)\n",
    "\n",
    "#         # Output layers\n",
    "#         self.final_conv1 = nn.Conv1d(num_channels, num_channels, 1)\n",
    "#         self.final_conv2 = nn.Conv1d(num_channels, audio_channels, 1)\n",
    "\n",
    "#         # Additional residual connection across the network\n",
    "#         self.residual_conv = nn.Conv1d(num_channels, num_channels, 1)\n",
    "\n",
    "#     def forward(self, audio, spectrogram):\n",
    "#         print(\"Original audio shape:\", audio.shape)\n",
    "#         print(\"Original spectrogram shape:\", spectrogram.shape)\n",
    "        \n",
    "#         audio_input = F.relu(self.audio_conv(audio))\n",
    "#         spectrogram_input = F.relu(self.spectrogram_conv(spectrogram))\n",
    "#         audio = audio_input\n",
    "#         spectrogram = spectrogram_input\n",
    "        \n",
    "#         print(\"Audio shape:\", audio.shape)\n",
    "#         print(\"Spectrogram shape:\", spectrogram.shape)\n",
    "\n",
    "#         audio_skip = 0\n",
    "#         spectrogram_skip = 0\n",
    "\n",
    "#         # Process through dilated convolutions with residual and skip connections\n",
    "#         for audio_conv, audio_skip_conv, spectro_conv, spectro_skip_conv in zip(self.audio_dilated_convs, self.audio_skip_convs, self.spectrogram_dilated_convs, self.spectrogram_skip_convs):\n",
    "#             print(\"Audio conv shape:\", audio_conv.shape)\n",
    "#             print(\"Audio skip conv shape:\", audio_skip_conv.shape)\n",
    "#             print(\"Spectrogram conv shape:\", spectro_conv.shape)\n",
    "#             print(\"Spectrogram skip conv shape:\", spectro_skip_conv.shape)\n",
    "#             tmp = F.relu(audio_conv(audio))\n",
    "#             print(\"relu shape:\", tmp.shape)\n",
    "#             print(\"audio shape:\", audio.shape)\n",
    "#             audio = F.relu(audio_conv(audio))-1 + audio\n",
    "#             spectrogram = F.relu(spectro_conv(spectrogram)) + spectrogram\n",
    "#             audio_skip += audio_skip_conv(audio)\n",
    "#             spectrogram_skip += spectro_skip_conv(spectrogram)\n",
    "        \n",
    "#         print(\"Audio skip shape:\", audio_skip.shape)\n",
    "#         print(\"Spectrogram skip shape:\", spectrogram_skip.shape)\n",
    "\n",
    "#         # Combine using multi-head attention\n",
    "#         combined, _ = self.feature_attention(audio_skip.transpose(1, 2), spectrogram_skip.transpose(1, 2), spectrogram_skip.transpose(1, 2))\n",
    "#         combined = combined.transpose(1, 2)\n",
    "        \n",
    "#         print(\"Combined shape:\", combined.shape)\n",
    "\n",
    "#         # Transformer processing with residual connection\n",
    "#         combined = self.transformer(combined.transpose(1, 2)).transpose(1, 2) + self.residual_conv(combined)\n",
    "\n",
    "#         print(\"Combined shape Transformer:\", combined.shape)\n",
    "#         # Final processing\n",
    "#         x = F.relu(self.final_conv1(combined))\n",
    "#         x = self.final_conv2(x)\n",
    "\n",
    "#         return x\n",
    "    \n",
    "#     def generate_audio(self, audio, spectrogram, sample_no, device='cpu'):\n",
    "#         \"\"\"\n",
    "#         Generate audio using the model and save it to a directory.\n",
    "\n",
    "#         Args:\n",
    "#         audio (torch.Tensor): The input audio tensor.\n",
    "#         spectrogram (torch.Tensor): The input spectrogram tensor.\n",
    "#         sample_no (int): The sample number to append to the filename.\n",
    "#         device (str): The device to perform computation on.\n",
    "#         \"\"\"\n",
    "#         # Ensure the model is in evaluation mode\n",
    "#         self.eval()\n",
    "#         # Move inputs to the correct device\n",
    "#         audio = audio.to(device)\n",
    "#         spectrogram = spectrogram.to(device)\n",
    "#         # Generate audio using the forward method\n",
    "#         with torch.no_grad():\n",
    "#             generated_audio = self.forward(audio, spectrogram)\n",
    "#         # Ensure the output directory exists\n",
    "#         output_dir = 'gen_music'\n",
    "#         os.makedirs(output_dir, exist_ok=True)\n",
    "#         # Save the generated audio to a file\n",
    "#         output_path = os.path.join(output_dir, f'ultranwav_{sample_no}.wav')\n",
    "#         torch.save(generated_audio, output_path)\n",
    "#         print(f\"Generated audio saved to {output_path}\")\n",
    "#         # Optionally, return the path or the audio tensor for further use\n",
    "#         return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\rahat/.cache\\torch\\hub\\harritaylor_torchvggish_master\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load a pre-trained VGGish model for audio feature extraction\n",
    "vggish = torch.hub.load('harritaylor/torchvggish', 'vggish')\n",
    "\n",
    "# Define the Perceptual Loss using VGGish as the feature extractor\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.feature_extractor.eval()  # Set to evaluation mode\n",
    "\n",
    "    def forward(self, generated_audio, target_audio):\n",
    "        with torch.no_grad():\n",
    "            real_features = self.feature_extractor(target_audio)\n",
    "        generated_features = self.feature_extractor(generated_audio)\n",
    "        loss = F.l1_loss(generated_features, real_features)\n",
    "        return loss\n",
    "\n",
    "perceptual_loss = PerceptualLoss(vggish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleSpectrogramLoss(nn.Module):\n",
    "    def __init__(self, scales=[1024, 2048, 4096]):\n",
    "        super(MultiScaleSpectrogramLoss, self).__init__()\n",
    "        self.scales = scales\n",
    "\n",
    "    def forward(self, generated_audio, target_audio):\n",
    "        loss = 0\n",
    "        for scale in self.scales:\n",
    "            gen_spec = torch.stft(generated_audio, n_fft=scale, return_complex=True)\n",
    "            target_spec = torch.stft(target_audio, n_fft=scale, return_complex=True)\n",
    "            loss += F.l1_loss(gen_spec.abs(), target_spec.abs())\n",
    "        return loss / len(self.scales)\n",
    "\n",
    "spectrogram_loss = MultiScaleSpectrogramLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration, let's assume we have a simple CNN as a discriminator\n",
    "class SimpleAudioDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleAudioDiscriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, stride=1, padding=1)  # Changed to Conv1d\n",
    "        self.fc1 = nn.Linear(16 * 16, 1)  # Adjust size according to actual output dimensions\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "    def intermediate_forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return x\n",
    "\n",
    "discriminator = SimpleAudioDiscriminator()\n",
    "\n",
    "class FeatureMatchingLoss(nn.Module):\n",
    "    def __init__(self, discriminator):\n",
    "        super(FeatureMatchingLoss, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.discriminator.eval()\n",
    "\n",
    "    def forward(self, generated_audio, target_audio):\n",
    "        with torch.no_grad():\n",
    "            real_features = self.discriminator.intermediate_forward(target_audio)\n",
    "        generated_features = self.discriminator.intermediate_forward(generated_audio)\n",
    "        loss = F.l1_loss(generated_features, real_features)\n",
    "        return loss\n",
    "\n",
    "feature_matching_loss = FeatureMatchingLoss(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a composite loss\n",
    "class CompositeLoss(nn.Module):\n",
    "    def __init__(self, perceptual_loss, spectrogram_loss, feature_matching_loss):\n",
    "        super(CompositeLoss, self).__init__()\n",
    "        self.perceptual_loss = perceptual_loss\n",
    "        self.spectrogram_loss = spectrogram_loss\n",
    "        self.feature_matching_loss = feature_matching_loss\n",
    "\n",
    "    def forward(self, generated_audio, target_audio):\n",
    "        loss = (self.perceptual_loss(generated_audio, target_audio) +\n",
    "                self.spectrogram_loss(generated_audio, target_audio) +\n",
    "                self.feature_matching_loss(generated_audio, target_audio))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UltimateTransformerWaveNet(nn.Module):\n",
    "    def __init__(self, audio_channels=1, spectrogram_channels=1025, num_channels=64, kernel_size=2, num_blocks=4, num_layers=10, num_heads=8):\n",
    "        super(UltimateTransformerWaveNet, self).__init__()\n",
    "        self.audio_conv = nn.Conv1d(audio_channels, num_channels, kernel_size=1)\n",
    "        self.spectrogram_conv = nn.Conv1d(spectrogram_channels, num_channels, kernel_size=1)\n",
    "\n",
    "        # Dilated convolutions with residual and skip connections for both streams\n",
    "        self.audio_dilated_convs = nn.ModuleList()\n",
    "        self.spectrogram_dilated_convs = nn.ModuleList()\n",
    "        self.audio_skip_convs = nn.ModuleList()\n",
    "        self.spectrogram_skip_convs = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            dilation = 2 ** i\n",
    "            self.audio_dilated_convs.append(nn.Conv1d(num_channels, num_channels, kernel_size, dilation=dilation, padding=dilation, groups=num_channels//16))\n",
    "            self.spectrogram_dilated_convs.append(nn.Conv1d(num_channels, num_channels, kernel_size, dilation=dilation, padding=dilation, groups=num_channels//16))\n",
    "            self.audio_skip_convs.append(nn.Conv1d(num_channels, num_channels, 1))\n",
    "            self.spectrogram_skip_convs.append(nn.Conv1d(num_channels, num_channels, 1))\n",
    "\n",
    "        # Multi-head attention for combining features\n",
    "        self.feature_attention = nn.MultiheadAttention(embed_dim=num_channels, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "        # Transformer block with residual connection\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=num_channels, nhead=num_heads, dim_feedforward=num_channels * 4, batch_first=True),\n",
    "            num_layers=3)\n",
    "\n",
    "        # Output layers\n",
    "        self.final_conv1 = nn.Conv1d(num_channels, num_channels, 1)\n",
    "        self.final_conv2 = nn.Conv1d(num_channels, audio_channels, 1)\n",
    "\n",
    "        # Additional residual connection across the network\n",
    "        self.residual_conv = nn.Conv1d(num_channels, num_channels, 1)\n",
    "\n",
    "    def forward(self, audio, spectrogram):\n",
    "        # print(\"Original audio shape:\", audio.shape)\n",
    "        # print(\"Original spectrogram shape:\", spectrogram.shape)\n",
    "        \n",
    "        audio_input = F.relu(self.audio_conv(audio))\n",
    "        spectrogram_input = F.relu(self.spectrogram_conv(spectrogram))\n",
    "        audio = audio_input\n",
    "        spectrogram = spectrogram_input\n",
    "        \n",
    "        # print(\"Audio shape:\", audio.shape)\n",
    "        # print(\"Spectrogram shape:\", spectrogram.shape)\n",
    "\n",
    "        audio_skip = 0\n",
    "        spectrogram_skip = 0\n",
    "\n",
    "        # Process through dilated convolutions with residual and skip connections\n",
    "        i = 0\n",
    "        for audio_conv, audio_skip_conv, spectro_conv, spectro_skip_conv in zip(self.audio_dilated_convs, self.audio_skip_convs, self.spectrogram_dilated_convs, self.spectrogram_skip_convs):\n",
    "            # print(\"Audio conv shape:\", audio_conv.shape)\n",
    "            # print(\"Audio skip conv shape:\", audio_skip_conv.shape)\n",
    "            # print(\"Spectrogram conv shape:\", spectro_conv.shape)\n",
    "            # print(\"Spectrogram skip conv shape:\", spectro_skip_conv.shape)\n",
    "            t1 = F.relu(audio_conv(audio))\n",
    "            # print(\"relu shape:\", t1.shape)\n",
    "            # print(\"audio shape:\", audio.shape)\n",
    "            t1 = t1[:, :, : -(2**i)]\n",
    "            # print(\"Modified relu shape:\", t1.shape)\n",
    "            audio = t1 + audio\n",
    "            \n",
    "            t2 = F.relu(spectro_conv(spectrogram))\n",
    "            # print(\"Spectrogram conv shape:\", t2.shape)\n",
    "            # print(\"Spectrogram shape:\", spectrogram.shape)\n",
    "            t2 = t2[:, :, :-(2**i)]\n",
    "            # print(\"Modified Spectrogram conv shape:\", t2.shape)\n",
    "            \n",
    "            spectrogram = t2 + spectrogram\n",
    "            audio_skip += audio_skip_conv(audio)\n",
    "            spectrogram_skip += spectro_skip_conv(spectrogram)\n",
    "            i += 1\n",
    "        \n",
    "        # print(\"Audio skip shape:\", audio_skip.shape)\n",
    "        # print(\"Spectrogram skip shape:\", spectrogram_skip.shape)\n",
    "\n",
    "        # Combine using multi-head attention\n",
    "        combined, _ = self.feature_attention(audio_skip.transpose(1, 2), spectrogram_skip.transpose(1, 2), spectrogram_skip.transpose(1, 2))\n",
    "        combined = combined.transpose(1, 2)\n",
    "        \n",
    "        # print(\"Combined shape:\", combined.shape)\n",
    "\n",
    "        # Transformer processing with residual connection\n",
    "        combined = self.transformer(combined.transpose(1, 2)).transpose(1, 2) + self.residual_conv(combined)\n",
    "\n",
    "        # print(\"Combined shape Transformer:\", combined.shape)\n",
    "        # Final processing\n",
    "        x = F.relu(self.final_conv1(combined))\n",
    "        x = self.final_conv2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def generate_audio(self, audio, spectrogram, sample_no, device='cpu'):\n",
    "        \"\"\"\n",
    "        Generate audio using the model and save it to a directory.\n",
    "\n",
    "        Args:\n",
    "        audio (torch.Tensor): The input audio tensor.\n",
    "        spectrogram (torch.Tensor): The input spectrogram tensor.\n",
    "        sample_no (int): The sample number to append to the filename.\n",
    "        device (str): The device to perform computation on.\n",
    "        \"\"\"\n",
    "        # Ensure the model is in evaluation mode\n",
    "        self.eval()\n",
    "        # Move inputs to the correct device\n",
    "        audio = audio.to(device)\n",
    "        spectrogram = spectrogram.to(device)\n",
    "        # Generate audio using the forward method\n",
    "        with torch.no_grad():\n",
    "            generated_audio = self.forward(audio, spectrogram)\n",
    "        # Ensure the output directory exists\n",
    "        output_dir = 'gen_music'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        # Save the generated audio to a file\n",
    "        output_path = os.path.join(output_dir, f'ultranwav_{sample_no}.wav')\n",
    "        torch.save(generated_audio, output_path)\n",
    "        print(f\"Generated audio saved to {output_path}\")\n",
    "        # Optionally, return the path or the audio tensor for further use\n",
    "        return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import wandb  # Ensure wandb is imported if you're using it\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, epochs, device):\n",
    "    model.to(device)\n",
    "    print(device)\n",
    "    print(\"Training Begins!\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for i, (audio, spectrogram) in enumerate(train_loader):\n",
    "            audio, spectrogram = audio.to(device), spectrogram.to(device)\n",
    "            \n",
    "            if audio.dim() == 2:\n",
    "                audio = audio.unsqueeze(1)  # Add channel dimension\n",
    "            elif audio.dim() != 3:\n",
    "                raise ValueError(\"Audio input must be 2D or 3D tensor\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(audio, spectrogram)\n",
    "            loss = criterion(output, audio)  # Ensure the criterion is correctly defined for the expected output\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Log loss to wandb\n",
    "            wandb.log({\"train_loss\": loss.item()})\n",
    "            if i % 10 == 0:  # Log every 10 steps\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item()}\")\n",
    "\n",
    "            # Save model checkpoint periodically or based on performance\n",
    "            if i % 100 == 0:  # Save every 100 iterations\n",
    "                checkpoint_path = f'TW_Checkpoint/model_TW_{epoch}_{i}.pt'\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "            # Generate synthetic data and add to train_loader if needed\n",
    "            if i % 50 == 0:  # Generate synthetic data every 50 iterations\n",
    "                with torch.no_grad():\n",
    "                    synthetic_audio = model.generate_audio(audio, spectrogram, i, device)\n",
    "                    # Assuming train_loader.dataset is a list or supports append\n",
    "                    train_loader.dataset.append((synthetic_audio.detach(), spectrogram))\n",
    "\n",
    "        epoch_loss /= len(train_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Average Loss: {epoch_loss}\")\n",
    "        wandb.log({\"epoch_loss\": epoch_loss})\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for audio, spectrogram in val_loader:\n",
    "                audio, spectrogram = audio.to(device), spectrogram.to(device)\n",
    "                output = model(audio, spectrogram)\n",
    "                val_loss += criterion(output, audio).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        wandb.log({\"val_loss\": val_loss})\n",
    "        print(f\"Validation Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[188], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mUltimateTransformerWaveNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# print(model)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\orc\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\orc\\Lib\\site-packages\\torch\\nn\\modules\\module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\orc\\Lib\\site-packages\\torch\\nn\\modules\\module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\orc\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1154\u001b[0m             device,\n\u001b[0;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1156\u001b[0m             non_blocking,\n\u001b[0;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1158\u001b[0m         )\n\u001b[1;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False, num_workers=0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UltimateTransformerWaveNet().to(device)\n",
    "# print(model)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "# composite_loss = CompositeLoss(perceptual_loss, spectrogram_loss, feature_matching_loss)\n",
    "composite_loss = CompositeLoss(perceptual_loss, spectrogram_loss, feature_matching_loss)\n",
    "\n",
    "# train(model, train_loader, val_loader, optimizer, composite_loss, epochs=50, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Training Begins!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[189], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomposite_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[187], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, optimizer, criterion, epochs, device)\u001b[0m\n\u001b[0;32m     12\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (audio, spectrogram) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m---> 14\u001b[0m     audio, spectrogram \u001b[38;5;241m=\u001b[39m \u001b[43maudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, spectrogram\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m audio\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m     17\u001b[0m         audio \u001b[38;5;241m=\u001b[39m audio\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Add channel dimension\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, optimizer, composite_loss, epochs=50, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
