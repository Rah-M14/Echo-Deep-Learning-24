{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import wandb\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# wandb.init(project='UlTraNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(audio_path, sample_rate=22050, duration=5):\n",
    "    # Load audio file with librosa, automatically resampling to the given sample rate\n",
    "    audio, sr = librosa.load(audio_path, sr=sample_rate, duration=duration)\n",
    "    \n",
    "    # Calculate target number of samples\n",
    "    target_length = sample_rate * duration\n",
    "    \n",
    "    # Pad audio if it is shorter than the target length\n",
    "    if len(audio) < target_length:\n",
    "        padding = target_length - len(audio)\n",
    "        audio = np.pad(audio, (0, padding), mode='constant')\n",
    "    # Truncate audio if it is longer than the target length\n",
    "    elif len(audio) > target_length:\n",
    "        audio = audio[:target_length]\n",
    "    \n",
    "    return audio\n",
    "\n",
    "def get_spectrogram(audio, n_fft=2048, hop_length=512, max_length=130):\n",
    "    # Generate a spectrogram\n",
    "    spectrogram = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)\n",
    "    # Convert to magnitude (amplitude)\n",
    "    spectrogram = np.abs(spectrogram)\n",
    "    \n",
    "    # Pad or truncate the spectrogram to ensure all are the same length\n",
    "    if spectrogram.shape[1] < max_length:\n",
    "        padding = max_length - spectrogram.shape[1]\n",
    "        spectrogram = np.pad(spectrogram, ((0, 0), (0, padding)), mode='constant')\n",
    "    else:\n",
    "        spectrogram = spectrogram[:, :max_length]\n",
    "    \n",
    "    return spectrogram\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, root_dir, sample_rate=22050, n_fft=2048, hop_length=512, max_length=130):\n",
    "        self.root_dir = root_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.max_length = max_length\n",
    "        self.files = [os.path.join(dp, f) for dp, dn, filenames in os.walk(root_dir) for f in filenames if f.endswith('.mp3') or f.endswith('.wav')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.files[idx]\n",
    "        audio = load_audio(audio_path, self.sample_rate)\n",
    "        spectrogram = get_spectrogram(audio, self.n_fft, self.hop_length, self.max_length)\n",
    "        return audio, spectrogram\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "\n",
    "    dataset = AudioDataset(root_dir='DATA')\n",
    "    loader = DataLoader(dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.01970823, -0.00225531, -0.03690785, ...,  0.02060048,\n",
      "       -0.00064142, -0.02519251], dtype=float32), array([[5.7437736e-01, 3.6425254e-01, 1.2862755e-01, ..., 7.2768354e-01,\n",
      "        2.6252899e-01, 3.3587483e-01],\n",
      "       [5.1895320e-01, 4.5031342e-01, 1.3857873e-01, ..., 5.0968748e-01,\n",
      "        4.5548519e-01, 5.7975030e-01],\n",
      "       [3.0562350e-01, 3.7505564e-01, 2.0325445e-01, ..., 3.5334751e-01,\n",
      "        4.1140336e-01, 9.5875704e-01],\n",
      "       ...,\n",
      "       [1.5232026e-03, 7.0222467e-04, 9.3422665e-07, ..., 2.8230016e-07,\n",
      "        1.7090463e-07, 1.6775441e-07],\n",
      "       [1.5125329e-03, 6.9691899e-04, 4.7191645e-07, ..., 1.4758260e-07,\n",
      "        2.3828001e-07, 3.1422653e-07],\n",
      "       [1.5089464e-03, 6.9499249e-04, 5.7717961e-07, ..., 3.2488643e-07,\n",
      "        2.2508639e-08, 2.8777606e-07]], dtype=float32))\n",
      "[ 0.01970823 -0.00225531 -0.03690785 ...  0.02060048 -0.00064142\n",
      " -0.02519251]\n",
      "(110250,)\n",
      "[[5.7437736e-01 3.6425254e-01 1.2862755e-01 ... 7.2768354e-01\n",
      "  2.6252899e-01 3.3587483e-01]\n",
      " [5.1895320e-01 4.5031342e-01 1.3857873e-01 ... 5.0968748e-01\n",
      "  4.5548519e-01 5.7975030e-01]\n",
      " [3.0562350e-01 3.7505564e-01 2.0325445e-01 ... 3.5334751e-01\n",
      "  4.1140336e-01 9.5875704e-01]\n",
      " ...\n",
      " [1.5232026e-03 7.0222467e-04 9.3422665e-07 ... 2.8230016e-07\n",
      "  1.7090463e-07 1.6775441e-07]\n",
      " [1.5125329e-03 6.9691899e-04 4.7191645e-07 ... 1.4758260e-07\n",
      "  2.3828001e-07 3.1422653e-07]\n",
      " [1.5089464e-03 6.9499249e-04 5.7717961e-07 ... 3.2488643e-07\n",
      "  2.2508639e-08 2.8777606e-07]]\n",
      "(1025, 130)\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])\n",
    "print((dataset[0][0]))\n",
    "print(dataset[0][0].shape)\n",
    "    \n",
    "print(dataset[0][1])\n",
    "print(dataset[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(total_size * train_ratio)\n",
    "    val_size = int(total_size * val_ratio)\n",
    "    test_size = total_size - train_size - val_size  # Ensure all data is used\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "data_folder_path = 'DATA'\n",
    "# dataset = AudioDataset(root_dir=data_folder_path)\n",
    "\n",
    "# Assuming 'dataset' is an instance of AudioDataset\n",
    "train_dataset, val_dataset, test_dataset = split_dataset(dataset)\n",
    "\n",
    "# Create DataLoaders for each dataset split\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in dataset: 407\n",
      "Single batch loaded successfully: [tensor([[-4.9719e-02, -1.0606e-01, -1.3332e-01,  ...,  3.5279e-01,\n",
      "          1.7849e-01,  4.1908e-02],\n",
      "        [ 5.0701e-02,  7.9188e-02,  7.8954e-02,  ...,  3.0429e-02,\n",
      "          4.0938e-02,  2.9423e-02],\n",
      "        [-4.6979e-02, -7.3127e-02, -6.4071e-02,  ..., -6.5827e-02,\n",
      "         -1.3733e-01, -2.2772e-01],\n",
      "        ...,\n",
      "        [ 1.2095e-05,  5.3243e-06,  1.6990e-05,  ...,  2.0313e-02,\n",
      "          1.1765e-02,  1.0368e-02],\n",
      "        [ 1.3617e-02,  5.9560e-02,  8.5912e-02,  ...,  5.8365e-01,\n",
      "          4.9166e-01,  4.2171e-01],\n",
      "        [-6.9656e-02, -1.2492e-01, -1.1681e-01,  ...,  4.7795e-01,\n",
      "          4.3123e-01,  4.2015e-01]]), tensor([[[4.1046e+00, 2.1018e+00, 5.9732e-02,  ..., 1.9285e-02,\n",
      "          2.8772e-02, 1.5061e-01],\n",
      "         [4.0843e+00, 2.0338e+00, 2.3267e-02,  ..., 3.0185e-02,\n",
      "          5.0452e-02, 1.5334e-01],\n",
      "         [4.0597e+00, 2.0199e+00, 6.7382e-03,  ..., 3.4802e-02,\n",
      "          5.0014e-02, 1.7139e-01],\n",
      "         ...,\n",
      "         [2.9861e-03, 1.5481e-03, 5.5786e-07,  ..., 4.5428e-07,\n",
      "          8.6480e-07, 5.0551e-07],\n",
      "         [2.9749e-03, 1.5429e-03, 7.0330e-07,  ..., 7.3622e-07,\n",
      "          5.8927e-07, 2.3440e-07],\n",
      "         [2.9711e-03, 1.5408e-03, 1.1047e-06,  ..., 8.8746e-10,\n",
      "          4.7305e-07, 1.2357e-07]],\n",
      "\n",
      "        [[1.5388e+00, 2.8530e-01, 5.5909e-01,  ..., 1.3228e+00,\n",
      "          1.1986e+00, 1.0428e+00],\n",
      "         [1.8488e+00, 1.2702e+00, 4.0526e-01,  ..., 4.4122e-01,\n",
      "          7.7503e-01, 6.9994e-01],\n",
      "         [2.2258e+00, 1.5172e+00, 1.0838e-01,  ..., 6.5266e-01,\n",
      "          6.7488e-01, 3.8491e-01],\n",
      "         ...,\n",
      "         [2.3034e-03, 1.2238e-03, 5.3613e-07,  ..., 3.4296e-08,\n",
      "          4.1093e-07, 2.4056e-07],\n",
      "         [2.2832e-03, 1.2155e-03, 1.5504e-06,  ..., 4.9501e-07,\n",
      "          6.5319e-07, 3.3612e-07],\n",
      "         [2.2767e-03, 1.2144e-03, 4.0182e-07,  ..., 1.4609e-07,\n",
      "          6.8380e-07, 3.7665e-07]],\n",
      "\n",
      "        [[3.6208e-01, 1.0213e-01, 1.7585e-01,  ..., 2.0160e+01,\n",
      "          1.0781e+01, 3.4136e+00],\n",
      "         [5.3289e-01, 2.7333e-01, 2.8360e-01,  ..., 1.5778e+01,\n",
      "          6.2479e+00, 2.0711e+00],\n",
      "         [7.2255e-01, 5.6856e-01, 2.6443e-01,  ..., 8.5498e+00,\n",
      "          3.1244e+00, 6.8043e-01],\n",
      "         ...,\n",
      "         [3.0794e-03, 1.5958e-03, 7.2193e-07,  ..., 6.9486e-07,\n",
      "          1.7464e-07, 1.7053e-07],\n",
      "         [3.0687e-03, 1.5905e-03, 7.7232e-07,  ..., 7.0750e-07,\n",
      "          4.6486e-07, 3.5737e-07],\n",
      "         [3.0651e-03, 1.5888e-03, 7.2948e-07,  ..., 4.8927e-07,\n",
      "          2.3923e-07, 6.3244e-07]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.3074e-02, 2.3433e-02, 2.5025e-02,  ..., 7.2673e-02,\n",
      "          1.9053e-01, 2.2360e-01],\n",
      "         [1.0671e-02, 1.3756e-02, 1.2935e-02,  ..., 9.9944e-02,\n",
      "          1.0413e-01, 9.9753e-02],\n",
      "         [6.0015e-03, 1.8619e-03, 6.7944e-04,  ..., 3.3812e-01,\n",
      "          2.2355e-01, 2.8163e-01],\n",
      "         ...,\n",
      "         [5.2144e-06, 2.4982e-06, 3.4255e-09,  ..., 2.2239e-07,\n",
      "          1.5455e-07, 6.6715e-08],\n",
      "         [5.2043e-06, 2.4954e-06, 1.4893e-09,  ..., 1.3844e-07,\n",
      "          4.8619e-08, 6.6325e-08],\n",
      "         [5.2031e-06, 2.4985e-06, 8.1356e-10,  ..., 1.7134e-07,\n",
      "          1.0959e-07, 7.9038e-08]],\n",
      "\n",
      "        [[3.7151e+00, 2.2842e+00, 6.5748e-01,  ..., 4.5808e-01,\n",
      "          1.2547e+00, 2.4876e+00],\n",
      "         [3.8317e+00, 1.3516e+00, 5.7696e-01,  ..., 8.7945e-01,\n",
      "          1.2337e+00, 2.0282e+00],\n",
      "         [3.8876e+00, 2.3665e+00, 1.0324e+00,  ..., 7.5669e-01,\n",
      "          1.2676e-01, 1.1615e+00],\n",
      "         ...,\n",
      "         [7.4133e-03, 3.7424e-03, 1.1488e-06,  ..., 1.2598e-05,\n",
      "          1.6099e-05, 1.3699e-05],\n",
      "         [7.3969e-03, 3.7282e-03, 5.4076e-06,  ..., 1.0879e-05,\n",
      "          1.7440e-05, 1.1022e-05],\n",
      "         [7.3917e-03, 3.7227e-03, 3.3227e-06,  ..., 9.9093e-06,\n",
      "          1.4687e-05, 9.8536e-06]],\n",
      "\n",
      "        [[8.8016e+00, 4.6764e+00, 4.0268e-01,  ..., 3.1991e-02,\n",
      "          3.7059e-01, 2.3470e-01],\n",
      "         [8.8575e+00, 4.1533e+00, 6.0976e-01,  ..., 1.7858e+00,\n",
      "          1.2274e+00, 4.0584e-01],\n",
      "         [1.2245e+01, 5.5322e+00, 4.3788e-01,  ..., 2.1471e+00,\n",
      "          8.4907e-01, 1.6399e+00],\n",
      "         ...,\n",
      "         [7.0641e-03, 3.5987e-03, 1.4275e-06,  ..., 3.0093e-06,\n",
      "          3.7855e-06, 2.6502e-06],\n",
      "         [7.0534e-03, 3.5939e-03, 5.7551e-07,  ..., 2.5980e-06,\n",
      "          3.2027e-06, 2.0768e-06],\n",
      "         [7.0492e-03, 3.5905e-03, 1.7612e-06,  ..., 2.0095e-06,\n",
      "          3.5020e-06, 2.6600e-06]]])]\n"
     ]
    }
   ],
   "source": [
    "# Check if the dataset is correctly set up\n",
    "print(\"Number of samples in dataset:\", len(train_dataset))\n",
    "\n",
    "# Create a DataLoader instance (make sure parameters like batch_size are set correctly)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Try to fetch a single batch to see if it works\n",
    "try:\n",
    "    data = next(iter(train_loader))\n",
    "    print(\"Single batch loaded successfully:\", data)\n",
    "except Exception as e:\n",
    "    print(\"Failed to load a batch:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 407\n",
      "Validation set size: 87\n",
      "Test set size: 88\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\rahat/.cache\\torch\\hub\\harritaylor_torchvggish_master\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load a pre-trained VGGish model for audio feature extraction\n",
    "vggish = torch.hub.load('harritaylor/torchvggish', 'vggish')\n",
    "\n",
    "# Define the Perceptual Loss using VGGish as the feature extractor\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.feature_extractor.eval()  # Set to evaluation mode\n",
    "\n",
    "    def forward(self, generated_audio, target_audio):\n",
    "        with torch.no_grad():\n",
    "            real_features = self.feature_extractor(target_audio)\n",
    "        generated_features = self.feature_extractor(generated_audio)\n",
    "        loss = F.l1_loss(generated_features, real_features)\n",
    "        return loss\n",
    "\n",
    "perceptual_loss = PerceptualLoss(vggish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleSpectrogramLoss(nn.Module):\n",
    "    def __init__(self, scales=[1024, 2048, 4096]):\n",
    "        super(MultiScaleSpectrogramLoss, self).__init__()\n",
    "        self.scales = scales\n",
    "\n",
    "    def forward(self, generated_audio, target_audio):\n",
    "        loss = 0\n",
    "        for scale in self.scales:\n",
    "            gen_spec = torch.stft(generated_audio, n_fft=scale, return_complex=True)\n",
    "            target_spec = torch.stft(target_audio, n_fft=scale, return_complex=True)\n",
    "            loss += F.l1_loss(gen_spec.abs(), target_spec.abs())\n",
    "        return loss / len(self.scales)\n",
    "\n",
    "spectrogram_loss = MultiScaleSpectrogramLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration, let's assume we have a simple CNN as a discriminator\n",
    "class SimpleAudioDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleAudioDiscriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, stride=1, padding=1)  # Changed to Conv1d\n",
    "        self.fc1 = nn.Linear(16 * 16, 1)  # Adjust size according to actual output dimensions\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "    def intermediate_forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return x\n",
    "\n",
    "discriminator = SimpleAudioDiscriminator()\n",
    "\n",
    "class FeatureMatchingLoss(nn.Module):\n",
    "    def __init__(self, discriminator):\n",
    "        super(FeatureMatchingLoss, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.discriminator.eval()\n",
    "\n",
    "    def forward(self, generated_audio, target_audio):\n",
    "        with torch.no_grad():\n",
    "            real_features = self.discriminator.intermediate_forward(target_audio)\n",
    "        generated_features = self.discriminator.intermediate_forward(generated_audio)\n",
    "        loss = F.l1_loss(generated_features, real_features)\n",
    "        return loss\n",
    "\n",
    "feature_matching_loss = FeatureMatchingLoss(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a composite loss\n",
    "class CompositeLoss(nn.Module):\n",
    "    def __init__(self, perceptual_loss, spectrogram_loss, feature_matching_loss):\n",
    "        super(CompositeLoss, self).__init__()\n",
    "        self.perceptual_loss = perceptual_loss\n",
    "        self.spectrogram_loss = spectrogram_loss\n",
    "        self.feature_matching_loss = feature_matching_loss\n",
    "\n",
    "    def forward(self, generated_audio, target_audio):\n",
    "        loss = (self.perceptual_loss(generated_audio, target_audio) +\n",
    "                self.spectrogram_loss(generated_audio, target_audio) +\n",
    "                self.feature_matching_loss(generated_audio, target_audio))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UltimateTransformerWaveNet(nn.Module):\n",
    "    def __init__(self, audio_channels=1, spectrogram_channels=1025, num_channels=64, kernel_size=2, num_blocks=4, num_layers=10, num_heads=8):\n",
    "        super(UltimateTransformerWaveNet, self).__init__()\n",
    "        self.audio_conv = nn.Conv1d(audio_channels, num_channels, kernel_size=1)\n",
    "        self.spectrogram_conv = nn.Conv1d(spectrogram_channels, num_channels, kernel_size=1)\n",
    "\n",
    "        # Dilated convolutions with residual and skip connections for both streams\n",
    "        self.audio_dilated_convs = nn.ModuleList()\n",
    "        self.spectrogram_dilated_convs = nn.ModuleList()\n",
    "        self.audio_skip_convs = nn.ModuleList()\n",
    "        self.spectrogram_skip_convs = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            dilation = 2 ** i\n",
    "            self.audio_dilated_convs.append(nn.Conv1d(num_channels, num_channels, kernel_size, dilation=dilation, padding=dilation, groups=num_channels//16))\n",
    "            self.spectrogram_dilated_convs.append(nn.Conv1d(num_channels, num_channels, kernel_size, dilation=dilation, padding=dilation, groups=num_channels//16))\n",
    "            self.audio_skip_convs.append(nn.Conv1d(num_channels, num_channels, 1))\n",
    "            self.spectrogram_skip_convs.append(nn.Conv1d(num_channels, num_channels, 1))\n",
    "\n",
    "        # Multi-head attention for combining features\n",
    "        self.feature_attention = nn.MultiheadAttention(embed_dim=num_channels, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "        # Transformer block with residual connection\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=num_channels, nhead=num_heads, dim_feedforward=num_channels * 4, batch_first=True),\n",
    "            num_layers=3)\n",
    "\n",
    "        # Output layers\n",
    "        self.final_conv1 = nn.Conv1d(num_channels, num_channels, 1)\n",
    "        self.final_conv2 = nn.Conv1d(num_channels, audio_channels, 1)\n",
    "\n",
    "        # Additional residual connection across the network\n",
    "        self.residual_conv = nn.Conv1d(num_channels, num_channels, 1)\n",
    "\n",
    "    def forward(self, audio, spectrogram):\n",
    "        # print(\"Original audio shape:\", audio.shape)\n",
    "        # print(\"Original spectrogram shape:\", spectrogram.shape)\n",
    "        \n",
    "        audio_input = F.relu(self.audio_conv(audio))\n",
    "        spectrogram_input = F.relu(self.spectrogram_conv(spectrogram))\n",
    "        audio = audio_input\n",
    "        spectrogram = spectrogram_input\n",
    "        \n",
    "        # print(\"Audio shape:\", audio.shape)\n",
    "        # print(\"Spectrogram shape:\", spectrogram.shape)\n",
    "\n",
    "        audio_skip = 0\n",
    "        spectrogram_skip = 0\n",
    "\n",
    "        # Process through dilated convolutions with residual and skip connections\n",
    "        i = 0\n",
    "        for audio_conv, audio_skip_conv, spectro_conv, spectro_skip_conv in zip(self.audio_dilated_convs, self.audio_skip_convs, self.spectrogram_dilated_convs, self.spectrogram_skip_convs):\n",
    "            # print(\"Audio conv shape:\", audio_conv.shape)\n",
    "            # print(\"Audio skip conv shape:\", audio_skip_conv.shape)\n",
    "            # print(\"Spectrogram conv shape:\", spectro_conv.shape)\n",
    "            # print(\"Spectrogram skip conv shape:\", spectro_skip_conv.shape)\n",
    "            t1 = F.relu(audio_conv(audio))\n",
    "            # print(\"relu shape:\", t1.shape)\n",
    "            # print(\"audio shape:\", audio.shape)\n",
    "            t1 = t1[:, :, : -(2**i)]\n",
    "            # print(\"Modified relu shape:\", t1.shape)\n",
    "            audio = t1 + audio\n",
    "            \n",
    "            t2 = F.relu(spectro_conv(spectrogram))\n",
    "            # print(\"Spectrogram conv shape:\", t2.shape)\n",
    "            # print(\"Spectrogram shape:\", spectrogram.shape)\n",
    "            t2 = t2[:, :, :-(2**i)]\n",
    "            # print(\"Modified Spectrogram conv shape:\", t2.shape)\n",
    "            \n",
    "            spectrogram = t2 + spectrogram\n",
    "            audio_skip += audio_skip_conv(audio)\n",
    "            spectrogram_skip += spectro_skip_conv(spectrogram)\n",
    "            i += 1\n",
    "        \n",
    "        # print(\"Audio skip shape:\", audio_skip.shape)\n",
    "        # print(\"Spectrogram skip shape:\", spectrogram_skip.shape)\n",
    "\n",
    "        # Combine using multi-head attention\n",
    "        combined, _ = self.feature_attention(audio_skip.transpose(1, 2), spectrogram_skip.transpose(1, 2), spectrogram_skip.transpose(1, 2))\n",
    "        combined = combined.transpose(1, 2)\n",
    "        \n",
    "        # print(\"Combined shape:\", combined.shape)\n",
    "\n",
    "        # Transformer processing with residual connection\n",
    "        combined = self.transformer(combined.transpose(1, 2)).transpose(1, 2) + self.residual_conv(combined)\n",
    "\n",
    "        # print(\"Combined shape Transformer:\", combined.shape)\n",
    "        # Final processing\n",
    "        x = F.relu(self.final_conv1(combined))\n",
    "        x = self.final_conv2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def generate_audio(self, audio, spectrogram, sample_no, device):\n",
    "        \"\"\"\n",
    "        Generate audio using the model and save it to a directory.\n",
    "\n",
    "        Args:\n",
    "        audio (torch.Tensor): The input audio tensor.\n",
    "        spectrogram (torch.Tensor): The input spectrogram tensor.\n",
    "        sample_no (int): The sample number to append to the filename.\n",
    "        device (str): The device to perform computation on.\n",
    "        \"\"\"\n",
    "        # Ensure the model is in evaluation mode\n",
    "        self.eval()\n",
    "        # Move inputs to the correct device\n",
    "        audio = audio.to(device)\n",
    "        spectrogram = spectrogram.to(device)\n",
    "        # Generate audio using the forward method\n",
    "        with torch.no_grad():\n",
    "            generated_audio = self.forward(audio, spectrogram)\n",
    "        # Ensure the output directory exists\n",
    "        output_dir = 'gen_music'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        # Save the generated audio to a file\n",
    "        output_path = os.path.join(output_dir, f'ultranwav_{sample_no}.wav')\n",
    "        torch.save(generated_audio, output_path)\n",
    "        print(f\"Generated audio saved to {output_path}\")\n",
    "        # Optionally, return the path or the audio tensor for further use\n",
    "        return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import wandb  # Ensure wandb is imported if you're using it\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, epochs, device):\n",
    "    model.to(device)\n",
    "    print(device)\n",
    "    print(\"Training Begins!\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for i, (audio, spectrogram) in enumerate(train_loader):\n",
    "            audio, spectrogram = audio.to(device), spectrogram.to(device)\n",
    "            \n",
    "            if audio.dim() == 2:\n",
    "                audio = audio.unsqueeze(1)  # Add channel dimension\n",
    "            elif audio.dim() != 3:\n",
    "                raise ValueError(\"Audio input must be 2D or 3D tensor\")\n",
    "            \n",
    "            window = torch.hann_window(1024, device=device)\n",
    "            spectrogram = torch.stft(spectrogram.squeeze(1), n_fft=1024, hop_length=256, win_length=1024, window=window, return_complex=True)\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(audio, spectrogram)\n",
    "            loss = criterion(output, audio)  # Ensure the criterion is correctly defined for the expected output\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Log loss to wandb\n",
    "            wandb.log({\"train_loss\": loss.item()})\n",
    "            if i % 10 == 0:  # Log every 10 steps\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item()}\")\n",
    "\n",
    "            # Save model checkpoint periodically or based on performance\n",
    "            if i % 100 == 0:  # Save every 100 iterations\n",
    "                checkpoint_path = f'TW_Checkpoint/model_TW_{epoch}_{i}.pt'\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "            # # Generate synthetic data and add to train_loader if needed\n",
    "            # if i % 50 == 0:  # Generate synthetic data every 50 iterations\n",
    "            #     with torch.no_grad():\n",
    "            #         synthetic_audio = model.generate_audio(audio, spectrogram, i, device)\n",
    "            #         # Assuming train_loader.dataset is a list or supports append\n",
    "            #         train_loader.dataset.append((synthetic_audio.detach(), spectrogram))\n",
    "\n",
    "        epoch_loss /= len(train_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Average Loss: {epoch_loss}\")\n",
    "        wandb.log({\"epoch_loss\": epoch_loss})\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for audio, spectrogram in val_loader:\n",
    "                audio, spectrogram = audio.to(device), spectrogram.to(device)\n",
    "                output = model(audio, spectrogram)\n",
    "                val_loss += criterion(output, audio).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        wandb.log({\"val_loss\": val_loss})\n",
    "        print(f\"Validation Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UltimateTransformerWaveNet(\n",
      "  (audio_conv): Conv1d(1, 64, kernel_size=(1,), stride=(1,))\n",
      "  (spectrogram_conv): Conv1d(1025, 64, kernel_size=(1,), stride=(1,))\n",
      "  (audio_dilated_convs): ModuleList(\n",
      "    (0): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(1,), groups=4)\n",
      "    (1): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,), groups=4)\n",
      "    (2): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(4,), dilation=(4,), groups=4)\n",
      "    (3): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(8,), dilation=(8,), groups=4)\n",
      "    (4): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(16,), dilation=(16,), groups=4)\n",
      "    (5): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(32,), dilation=(32,), groups=4)\n",
      "    (6): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(64,), dilation=(64,), groups=4)\n",
      "    (7): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(128,), dilation=(128,), groups=4)\n",
      "    (8): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(256,), dilation=(256,), groups=4)\n",
      "    (9): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(512,), dilation=(512,), groups=4)\n",
      "  )\n",
      "  (spectrogram_dilated_convs): ModuleList(\n",
      "    (0): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(1,), groups=4)\n",
      "    (1): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,), groups=4)\n",
      "    (2): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(4,), dilation=(4,), groups=4)\n",
      "    (3): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(8,), dilation=(8,), groups=4)\n",
      "    (4): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(16,), dilation=(16,), groups=4)\n",
      "    (5): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(32,), dilation=(32,), groups=4)\n",
      "    (6): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(64,), dilation=(64,), groups=4)\n",
      "    (7): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(128,), dilation=(128,), groups=4)\n",
      "    (8): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(256,), dilation=(256,), groups=4)\n",
      "    (9): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(512,), dilation=(512,), groups=4)\n",
      "  )\n",
      "  (audio_skip_convs): ModuleList(\n",
      "    (0-9): 10 x Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (spectrogram_skip_convs): ModuleList(\n",
      "    (0-9): 10 x Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (feature_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_conv1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "  (final_conv2): Conv1d(64, 1, kernel_size=(1,), stride=(1,))\n",
      "  (residual_conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False, num_workers=0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UltimateTransformerWaveNet().to(device)\n",
    "print(model)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "# composite_loss = CompositeLoss(perceptual_loss, spectrogram_loss, feature_matching_loss)\n",
    "composite_loss = CompositeLoss(perceptual_loss, spectrogram_loss, feature_matching_loss)\n",
    "\n",
    "# train(model, train_loader, val_loader, optimizer, composite_loss, epochs=50, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(model, train_loader, val_loader, optimizer, composite_loss, epochs=50, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: UlTranNet_Checkpoints\\model_TW_0.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UltimateTransformerWaveNet(\n",
       "  (audio_conv): Conv1d(1, 64, kernel_size=(1,), stride=(1,))\n",
       "  (spectrogram_conv): Conv1d(1025, 64, kernel_size=(1,), stride=(1,))\n",
       "  (audio_dilated_convs): ModuleList(\n",
       "    (0): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(1,), groups=4)\n",
       "    (1): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,), groups=4)\n",
       "    (2): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(4,), dilation=(4,), groups=4)\n",
       "    (3): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(8,), dilation=(8,), groups=4)\n",
       "    (4): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(16,), dilation=(16,), groups=4)\n",
       "    (5): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(32,), dilation=(32,), groups=4)\n",
       "    (6): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(64,), dilation=(64,), groups=4)\n",
       "    (7): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(128,), dilation=(128,), groups=4)\n",
       "    (8): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(256,), dilation=(256,), groups=4)\n",
       "    (9): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(512,), dilation=(512,), groups=4)\n",
       "  )\n",
       "  (spectrogram_dilated_convs): ModuleList(\n",
       "    (0): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(1,), groups=4)\n",
       "    (1): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,), groups=4)\n",
       "    (2): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(4,), dilation=(4,), groups=4)\n",
       "    (3): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(8,), dilation=(8,), groups=4)\n",
       "    (4): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(16,), dilation=(16,), groups=4)\n",
       "    (5): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(32,), dilation=(32,), groups=4)\n",
       "    (6): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(64,), dilation=(64,), groups=4)\n",
       "    (7): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(128,), dilation=(128,), groups=4)\n",
       "    (8): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(256,), dilation=(256,), groups=4)\n",
       "    (9): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=(512,), dilation=(512,), groups=4)\n",
       "  )\n",
       "  (audio_skip_convs): ModuleList(\n",
       "    (0-9): 10 x Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (spectrogram_skip_convs): ModuleList(\n",
       "    (0-9): 10 x Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (feature_attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_conv1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "  (final_conv2): Conv1d(64, 1, kernel_size=(1,), stride=(1,))\n",
       "  (residual_conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "def load_latest_checkpoint(checkpoint_dir):\n",
    "    checkpoint_files = [file for file in os.listdir(checkpoint_dir) if file.endswith('.pt')]\n",
    "    latest_file = max(checkpoint_files, key=lambda x: os.path.getctime(os.path.join(checkpoint_dir, x)))\n",
    "    latest_path = os.path.join(checkpoint_dir, latest_file)\n",
    "    print(f\"Loading checkpoint: {latest_path}\")\n",
    "    return torch.load(latest_path)\n",
    "\n",
    "checkpoint_dir = 'UlTranNet_Checkpoints'\n",
    "model = UltimateTransformerWaveNet()  # Assuming the model class is defined and imported\n",
    "latest_checkpoint = load_latest_checkpoint(checkpoint_dir)\n",
    "model.load_state_dict(latest_checkpoint)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model(model, test_loader, device):\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for audio, spectrogram in test_loader:\n",
    "#             audio, spectrogram = audio.to(device), spectrogram.to(device)\n",
    "#             if audio.dim() == 2:\n",
    "#                 audio = audio.unsqueeze(1)  # Add channel dimension\n",
    "#             elif audio.dim() != 3:\n",
    "#                 raise ValueError(\"Audio input must be 2D or 3D tensor\")\n",
    "            \n",
    "#             # window = torch.hann_window(1024, device=device)\n",
    "#             # spectrogram = torch.stft(spectrogram.squeeze(1), n_fft=1024, hop_length=256, win_length=1024, window=window, return_complex=True)\n",
    "#             output = model(audio, spectrogram)\n",
    "#             # Here you can add code to calculate any metrics or losses if needed\n",
    "#     print(\"Evaluation completed.\")\n",
    "\n",
    "# # Assuming test_loader is defined and device is set\n",
    "# evaluate_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rahat\\miniconda3\\envs\\orc\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "c:\\Users\\rahat\\miniconda3\\envs\\orc\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:720: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  return torch._transformer_encoder_layer_fwd(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage PESQ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_pesq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Assuming test_loader and device are defined\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[43mevaluate_model_simplified\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 26\u001b[0m, in \u001b[0;36mevaluate_model_simplified\u001b[1;34m(model, test_loader, device)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio input must be 2D or 3D tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m output \u001b[38;5;241m=\u001b[39m model(audio\u001b[38;5;241m.\u001b[39mto(device), spectrogram)\n\u001b[1;32m---> 26\u001b[0m pesq_score \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_pesq\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m22050\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m total_pesq \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pesq_score\n\u001b[0;32m     28\u001b[0m count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[24], line 8\u001b[0m, in \u001b[0;36mcalculate_pesq\u001b[1;34m(reference_audio, generated_audio, sample_rate)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_pesq\u001b[39m(reference_audio, generated_audio, sample_rate):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# PESQ expects audio data in the form of numpy arrays with int16 format.\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     ref_audio_int16 \u001b[38;5;241m=\u001b[39m (\u001b[43mreference_audio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m32767\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint16)\n\u001b[0;32m      9\u001b[0m     gen_audio_int16 \u001b[38;5;241m=\u001b[39m (generated_audio\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m32767\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint16)\n\u001b[0;32m     10\u001b[0m     score \u001b[38;5;241m=\u001b[39m pesq\u001b[38;5;241m.\u001b[39mpesq(sample_rate, ref_audio_int16, gen_audio_int16, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Wideband mode\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pesq\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "\n",
    "def calculate_pesq(reference_audio, generated_audio, sample_rate):\n",
    "    # PESQ expects audio data in the form of numpy arrays with int16 format.\n",
    "    ref_audio_int16 = (reference_audio.numpy() * 32767).astype(np.int16)\n",
    "    gen_audio_int16 = (generated_audio.numpy() * 32767).astype(np.int16)\n",
    "    score = pesq.pesq(sample_rate, ref_audio_int16, gen_audio_int16, 'wb')  # Wideband mode\n",
    "    return score\n",
    "\n",
    "def evaluate_model_simplified(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    total_pesq, count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for audio, spectrogram in test_loader:\n",
    "            audio, spectrogram = audio.to(device), spectrogram.to(device)\n",
    "            if audio.dim() == 2:\n",
    "                audio = audio.unsqueeze(1)  # Add channel dimension\n",
    "            elif audio.dim() != 3:\n",
    "                raise ValueError(\"Audio input must be 2D or 3D tensor\")\n",
    "     \n",
    "            output = model(audio.to(device), spectrogram)\n",
    "            pesq_score = calculate_pesq(audio, output, 22050)\n",
    "            total_pesq += pesq_score\n",
    "            count += 1\n",
    "    avg_pesq = total_pesq / count\n",
    "    print(f\"Average PESQ: {avg_pesq}\")\n",
    "\n",
    "# Assuming test_loader and device are defined\n",
    "evaluate_model_simplified(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pesq\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import random\n",
    "\n",
    "def calculate_pesq(reference_audio, generated_audio, sample_rate):\n",
    "    # PESQ expects audio data in the form of numpy arrays with int16 format.\n",
    "    ref_audio_int16 = (reference_audio.numpy() * 32767).astype(np.int16)\n",
    "    gen_audio_int16 = (generated_audio.numpy() * 32767).astype(np.int16)\n",
    "    score = pesq.pesq(sample_rate, ref_audio_int16, gen_audio_int16, 'wb')  # Wideband mode\n",
    "    return score\n",
    "\n",
    "def generate_and_save_wav(audio_tensor, filename, sample_rate=22050):\n",
    "    # Convert the tensor to numpy array and scale to int16\n",
    "    audio_numpy = (audio_tensor.numpy() * 32767).astype(np.int16)\n",
    "    # Write the WAV file\n",
    "    wavfile.write(filename, sample_rate, audio_numpy)\n",
    "    print(f\"Saved generated audio to {filename}\")\n",
    "\n",
    "def evaluate_and_generate_audio_random_sample(model, test_loader, device, save_path='generated_audio.wav'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    # Convert DataLoader to a list to randomly select a sample\n",
    "    test_samples = list(test_loader)\n",
    "    random_index = random.randint(0, len(test_samples) - 1)\n",
    "    audio, spectrogram = test_samples[random_index]\n",
    "    \n",
    "    audio, spectrogram = audio.to(device), spectrogram.to(device)\n",
    "    if audio.dim() == 2:\n",
    "        audio = audio.unsqueeze(1)  # Add channel dimension\n",
    "    elif audio.dim() != 3:\n",
    "        raise ValueError(\"Audio input must be 2D or 3D tensor\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(audio, spectrogram)\n",
    "        pesq_score = calculate_pesq(audio, output, 22050)\n",
    "        print(f\"PESQ Score: {pesq_score}\")\n",
    "\n",
    "        # Save the generated audio to a WAV file\n",
    "        generate_and_save_wav(output[0], save_path, 22050)\n",
    "\n",
    "# Assuming test_loader and device are defined\n",
    "evaluate_and_generate_audio_random_sample(model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
