{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrah-m\u001b[0m (\u001b[33mrebot\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\SEM6\\DL\\PROJ\\BEGIN\\wandb\\run-20240510_220938-31c2b5nq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rebot/UlTraNet/runs/31c2b5nq' target=\"_blank\">treasured-water-1</a></strong> to <a href='https://wandb.ai/rebot/UlTraNet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rebot/UlTraNet' target=\"_blank\">https://wandb.ai/rebot/UlTraNet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rebot/UlTraNet/runs/31c2b5nq' target=\"_blank\">https://wandb.ai/rebot/UlTraNet/runs/31c2b5nq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/rebot/UlTraNet/runs/31c2b5nq?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1645c1a3450>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import wandb\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "wandb.init(project='UlTraNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(audio_path, sample_rate=22050, duration=5):\n",
    "    # Load audio file with librosa, automatically resampling to the given sample rate\n",
    "    audio, sr = librosa.load(audio_path, sr=sample_rate, duration=duration)\n",
    "    \n",
    "    # Calculate target number of samples\n",
    "    target_length = sample_rate * duration\n",
    "    \n",
    "    # Pad audio if it is shorter than the target length\n",
    "    if len(audio) < target_length:\n",
    "        padding = target_length - len(audio)\n",
    "        audio = np.pad(audio, (0, padding), mode='constant')\n",
    "    # Truncate audio if it is longer than the target length\n",
    "    elif len(audio) > target_length:\n",
    "        audio = audio[:target_length]\n",
    "    \n",
    "    return audio\n",
    "\n",
    "def get_spectrogram(audio, n_fft=2048, hop_length=512, max_length=130):\n",
    "    # Generate a spectrogram\n",
    "    spectrogram = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)\n",
    "    # Convert to magnitude (amplitude)\n",
    "    spectrogram = np.abs(spectrogram)\n",
    "    \n",
    "    # Pad or truncate the spectrogram to ensure all are the same length\n",
    "    if spectrogram.shape[1] < max_length:\n",
    "        padding = max_length - spectrogram.shape[1]\n",
    "        spectrogram = np.pad(spectrogram, ((0, 0), (0, padding)), mode='constant')\n",
    "    else:\n",
    "        spectrogram = spectrogram[:, :max_length]\n",
    "    \n",
    "    return spectrogram\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, root_dir, sample_rate=22050, n_fft=2048, hop_length=512, max_length=130):\n",
    "        self.root_dir = root_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.max_length = max_length\n",
    "        self.files = [os.path.join(dp, f) for dp, dn, filenames in os.walk(root_dir) for f in filenames if f.endswith('.mp3') or f.endswith('.wav')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.files[idx]\n",
    "        audio = load_audio(audio_path, self.sample_rate)\n",
    "        spectrogram = get_spectrogram(audio, self.n_fft, self.hop_length, self.max_length)\n",
    "        return audio, spectrogram\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "\n",
    "    dataset = AudioDataset(root_dir='DATA')\n",
    "    loader = DataLoader(dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.01970823, -0.00225531, -0.03690785, ...,  0.02060048,\n",
      "       -0.00064142, -0.02519251], dtype=float32), array([[5.7437736e-01, 3.6425254e-01, 1.2862755e-01, ..., 7.2768354e-01,\n",
      "        2.6252899e-01, 3.3587483e-01],\n",
      "       [5.1895320e-01, 4.5031342e-01, 1.3857873e-01, ..., 5.0968748e-01,\n",
      "        4.5548519e-01, 5.7975030e-01],\n",
      "       [3.0562350e-01, 3.7505564e-01, 2.0325445e-01, ..., 3.5334751e-01,\n",
      "        4.1140336e-01, 9.5875704e-01],\n",
      "       ...,\n",
      "       [1.5232026e-03, 7.0222467e-04, 9.3422665e-07, ..., 2.8230016e-07,\n",
      "        1.7090463e-07, 1.6775441e-07],\n",
      "       [1.5125329e-03, 6.9691899e-04, 4.7191645e-07, ..., 1.4758260e-07,\n",
      "        2.3828001e-07, 3.1422653e-07],\n",
      "       [1.5089464e-03, 6.9499249e-04, 5.7717961e-07, ..., 3.2488643e-07,\n",
      "        2.2508639e-08, 2.8777606e-07]], dtype=float32))\n",
      "[ 0.01970823 -0.00225531 -0.03690785 ...  0.02060048 -0.00064142\n",
      " -0.02519251]\n",
      "(110250,)\n",
      "[[5.7437736e-01 3.6425254e-01 1.2862755e-01 ... 7.2768354e-01\n",
      "  2.6252899e-01 3.3587483e-01]\n",
      " [5.1895320e-01 4.5031342e-01 1.3857873e-01 ... 5.0968748e-01\n",
      "  4.5548519e-01 5.7975030e-01]\n",
      " [3.0562350e-01 3.7505564e-01 2.0325445e-01 ... 3.5334751e-01\n",
      "  4.1140336e-01 9.5875704e-01]\n",
      " ...\n",
      " [1.5232026e-03 7.0222467e-04 9.3422665e-07 ... 2.8230016e-07\n",
      "  1.7090463e-07 1.6775441e-07]\n",
      " [1.5125329e-03 6.9691899e-04 4.7191645e-07 ... 1.4758260e-07\n",
      "  2.3828001e-07 3.1422653e-07]\n",
      " [1.5089464e-03 6.9499249e-04 5.7717961e-07 ... 3.2488643e-07\n",
      "  2.2508639e-08 2.8777606e-07]]\n",
      "(1025, 130)\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])\n",
    "print((dataset[0][0]))\n",
    "print(dataset[0][0].shape)\n",
    "    \n",
    "print(dataset[0][1])\n",
    "print(dataset[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(total_size * train_ratio)\n",
    "    val_size = int(total_size * val_ratio)\n",
    "    test_size = total_size - train_size - val_size  # Ensure all data is used\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "data_folder_path = 'DATA'\n",
    "# dataset = AudioDataset(root_dir=data_folder_path)\n",
    "\n",
    "# Assuming 'dataset' is an instance of AudioDataset\n",
    "train_dataset, val_dataset, test_dataset = split_dataset(dataset)\n",
    "\n",
    "# Create DataLoaders for each dataset split\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in dataset: 407\n",
      "Single batch loaded successfully: [tensor([[ 0.0878,  0.1430,  0.0810,  ...,  0.0283,  0.0504,  0.0705],\n",
      "        [ 0.0167,  0.0532,  0.0776,  ..., -0.0841, -0.0784, -0.0551],\n",
      "        [-0.0312, -0.0445, -0.0273,  ..., -0.0202, -0.0229, -0.0380],\n",
      "        ...,\n",
      "        [-0.0153, -0.0630, -0.0978,  ...,  0.0057, -0.0427, -0.0331],\n",
      "        [ 0.0447,  0.1251, -0.0108,  ..., -0.1214, -0.1725, -0.2205],\n",
      "        [-0.0428, -0.0548, -0.0314,  ...,  0.0466,  0.0607,  0.1009]]), tensor([[[2.5160e+00, 1.0032e+00, 1.9823e-02,  ..., 2.8660e-01,\n",
      "          4.8994e-01, 2.2253e+00],\n",
      "         [3.0200e+00, 1.2976e+00, 4.0251e-01,  ..., 4.5947e-01,\n",
      "          1.2834e+00, 1.8520e+00],\n",
      "         [3.1917e+00, 3.2510e+00, 2.3933e+00,  ..., 7.7637e-01,\n",
      "          1.5572e+00, 1.3262e+00],\n",
      "         ...,\n",
      "         [8.2973e-03, 4.2435e-03, 2.7811e-06,  ..., 1.6842e-06,\n",
      "          3.2017e-06, 7.5442e-08],\n",
      "         [8.2788e-03, 4.2355e-03, 1.1172e-06,  ..., 1.5672e-06,\n",
      "          3.0027e-06, 1.6633e-07],\n",
      "         [8.2749e-03, 4.2338e-03, 6.3747e-07,  ..., 1.7645e-06,\n",
      "          3.1932e-06, 1.3832e-07]],\n",
      "\n",
      "        [[2.6177e-02, 1.4458e-01, 6.7864e-02,  ..., 2.0798e-01,\n",
      "          2.0009e-01, 1.9827e-01],\n",
      "         [4.5653e-02, 2.0469e-01, 3.2754e-01,  ..., 6.0812e-02,\n",
      "          1.3795e-01, 1.0253e-01],\n",
      "         [6.5191e-02, 2.2918e-01, 3.8164e-01,  ..., 5.3570e-02,\n",
      "          8.0887e-02, 4.4441e-02],\n",
      "         ...,\n",
      "         [1.6950e-03, 8.3704e-04, 1.1883e-06,  ..., 3.1006e-07,\n",
      "          2.7640e-07, 1.0075e-07],\n",
      "         [1.6944e-03, 8.3617e-04, 1.0335e-06,  ..., 2.7847e-07,\n",
      "          1.6280e-07, 2.9405e-07],\n",
      "         [1.6918e-03, 8.3582e-04, 1.3549e-06,  ..., 4.3597e-07,\n",
      "          8.0783e-07, 2.7620e-07]],\n",
      "\n",
      "        [[2.4974e+00, 1.1813e+00, 4.2396e-01,  ..., 5.8633e-01,\n",
      "          8.9236e-01, 6.6328e-01],\n",
      "         [2.5054e+00, 1.3986e+00, 4.7469e-01,  ..., 1.2975e+00,\n",
      "          1.4695e+00, 1.1027e+00],\n",
      "         [2.5382e+00, 1.1817e+00, 3.8646e-01,  ..., 9.1547e-01,\n",
      "          1.1143e+00, 8.7437e-01],\n",
      "         ...,\n",
      "         [3.6259e-03, 1.8583e-03, 8.0524e-07,  ..., 2.8367e-06,\n",
      "          2.8425e-06, 5.7680e-06],\n",
      "         [3.6172e-03, 1.8539e-03, 9.0587e-07,  ..., 1.5877e-06,\n",
      "          8.3189e-07, 1.4581e-06],\n",
      "         [3.6142e-03, 1.8524e-03, 7.0009e-07,  ..., 1.6726e-06,\n",
      "          6.4331e-07, 1.5726e-06]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[9.4775e-02, 8.7037e-02, 1.8231e-02,  ..., 1.0964e-01,\n",
      "          9.0262e-02, 7.5792e-02],\n",
      "         [7.2974e-02, 6.3143e-02, 9.4386e-02,  ..., 2.3226e-01,\n",
      "          1.4467e-01, 4.2463e-02],\n",
      "         [1.7543e-01, 2.0490e-01, 1.4024e-01,  ..., 2.7877e-01,\n",
      "          1.6385e-01, 9.3460e-02],\n",
      "         ...,\n",
      "         [8.6915e-04, 4.2708e-04, 1.2193e-07,  ..., 2.9904e-07,\n",
      "          3.0143e-07, 2.9003e-07],\n",
      "         [8.6931e-04, 4.2741e-04, 8.8758e-08,  ..., 2.4602e-07,\n",
      "          5.0179e-07, 2.3341e-07],\n",
      "         [8.6938e-04, 4.2732e-04, 4.8302e-07,  ..., 6.8260e-07,\n",
      "          4.9191e-07, 9.8955e-07]],\n",
      "\n",
      "        [[4.9271e-01, 1.0894e+00, 1.6352e-02,  ..., 1.0571e+00,\n",
      "          2.0863e-01, 3.6040e-02],\n",
      "         [1.2695e+00, 1.7260e+00, 1.0605e+00,  ..., 1.0973e+00,\n",
      "          7.0977e-02, 2.3817e-01],\n",
      "         [5.5302e-01, 2.6400e+00, 2.7326e+00,  ..., 1.3979e+00,\n",
      "          4.1007e-01, 7.5383e-01],\n",
      "         ...,\n",
      "         [1.1182e-01, 5.5187e-02, 1.8184e-06,  ..., 6.1785e-06,\n",
      "          2.2001e-06, 1.8475e-06],\n",
      "         [1.1172e-01, 5.5163e-02, 8.8740e-06,  ..., 1.3254e-06,\n",
      "          2.1359e-06, 1.6613e-06],\n",
      "         [1.1174e-01, 5.5160e-02, 7.8497e-06,  ..., 1.1262e-06,\n",
      "          1.0418e-06, 9.8863e-07]],\n",
      "\n",
      "        [[1.0652e+01, 2.2257e+01, 2.5395e+01,  ..., 4.0484e+01,\n",
      "          3.8219e+01, 4.0280e+01],\n",
      "         [8.7024e+00, 1.6149e+01, 1.0102e+01,  ..., 2.1162e+01,\n",
      "          1.7723e+01, 2.2817e+01],\n",
      "         [3.9212e+00, 9.9792e+00, 2.6621e+00,  ..., 8.1809e+00,\n",
      "          7.7432e+00, 8.6531e+00],\n",
      "         ...,\n",
      "         [8.8612e-03, 4.5034e-03, 1.9059e-06,  ..., 6.8676e-06,\n",
      "          7.4380e-06, 1.1227e-06],\n",
      "         [8.8436e-03, 4.4938e-03, 2.9149e-06,  ..., 7.5736e-06,\n",
      "          6.6472e-06, 4.3015e-06],\n",
      "         [8.8393e-03, 4.4918e-03, 1.6274e-06,  ..., 6.0497e-06,\n",
      "          6.6322e-06, 1.1322e-06]]])]\n"
     ]
    }
   ],
   "source": [
    "# Check if the dataset is correctly set up\n",
    "print(\"Number of samples in dataset:\", len(train_dataset))\n",
    "\n",
    "# Create a DataLoader instance (make sure parameters like batch_size are set correctly)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Try to fetch a single batch to see if it works\n",
    "try:\n",
    "    data = next(iter(train_loader))\n",
    "    print(\"Single batch loaded successfully:\", data)\n",
    "except Exception as e:\n",
    "    print(\"Failed to load a batch:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 407\n",
      "Validation set size: 87\n",
      "Test set size: 88\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\rahat/.cache\\torch\\hub\\harritaylor_torchvggish_master\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load a pre-trained VGGish model for audio feature extraction\n",
    "vggish = torch.hub.load('harritaylor/torchvggish', 'vggish')\n",
    "\n",
    "# Define the Perceptual Loss using VGGish as the feature extractor\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.feature_extractor.eval()  # Set to evaluation mode\n",
    "\n",
    "    def forward(self, generated_audio, target_audio):\n",
    "        with torch.no_grad():\n",
    "            real_features = self.feature_extractor(target_audio)\n",
    "        generated_features = self.feature_extractor(generated_audio)\n",
    "        loss = F.l1_loss(generated_features, real_features)\n",
    "        return loss\n",
    "\n",
    "perceptual_loss = PerceptualLoss(vggish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleSpectrogramLoss(nn.Module):\n",
    "    def __init__(self, scales=[1024, 2048, 4096]):\n",
    "        super(MultiScaleSpectrogramLoss, self).__init__()\n",
    "        self.scales = scales\n",
    "\n",
    "    def forward(self, generated_audio, target_audio):\n",
    "        loss = 0\n",
    "        for scale in self.scales:\n",
    "            gen_spec = torch.stft(generated_audio, n_fft=scale, return_complex=True)\n",
    "            target_spec = torch.stft(target_audio, n_fft=scale, return_complex=True)\n",
    "            loss += F.l1_loss(gen_spec.abs(), target_spec.abs())\n",
    "        return loss / len(self.scales)\n",
    "\n",
    "spectrogram_loss = MultiScaleSpectrogramLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration, let's assume we have a simple CNN as a discriminator\n",
    "class SimpleAudioDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleAudioDiscriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, stride=1, padding=1)  # Changed to Conv1d\n",
    "        self.fc1 = nn.Linear(16 * 16, 1)  # Adjust size according to actual output dimensions\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "    def intermediate_forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return x\n",
    "\n",
    "discriminator = SimpleAudioDiscriminator()\n",
    "\n",
    "class FeatureMatchingLoss(nn.Module):\n",
    "    def __init__(self, discriminator):\n",
    "        super(FeatureMatchingLoss, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.discriminator.eval()\n",
    "\n",
    "    def forward(self, generated_audio, target_audio):\n",
    "        with torch.no_grad():\n",
    "            real_features = self.discriminator.intermediate_forward(target_audio)\n",
    "        generated_features = self.discriminator.intermediate_forward(generated_audio)\n",
    "        loss = F.l1_loss(generated_features, real_features)\n",
    "        return loss\n",
    "\n",
    "feature_matching_loss = FeatureMatchingLoss(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a composite loss\n",
    "class CompositeLoss(nn.Module):\n",
    "    def __init__(self, perceptual_loss, spectrogram_loss, feature_matching_loss):\n",
    "        super(CompositeLoss, self).__init__()\n",
    "        self.perceptual_loss = perceptual_loss\n",
    "        self.spectrogram_loss = spectrogram_loss\n",
    "        self.feature_matching_loss = feature_matching_loss\n",
    "\n",
    "    def forward(self, generated_audio, target_audio):\n",
    "        loss = (self.perceptual_loss(generated_audio, target_audio) +\n",
    "                self.spectrogram_loss(generated_audio, target_audio) +\n",
    "                self.feature_matching_loss(generated_audio, target_audio))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UltimateTransformerWaveNet(nn.Module):\n",
    "    def __init__(self, audio_channels=1, spectrogram_channels=1025, num_channels=64, kernel_size=2, num_blocks=4, num_layers=10, num_heads=8):\n",
    "        super(UltimateTransformerWaveNet, self).__init__()\n",
    "        self.audio_conv = nn.Conv1d(audio_channels, num_channels, kernel_size=1)\n",
    "        self.spectrogram_conv = nn.Conv1d(spectrogram_channels, num_channels, kernel_size=1)\n",
    "\n",
    "        # Dilated convolutions with residual and skip connections for both streams\n",
    "        self.audio_dilated_convs = nn.ModuleList()\n",
    "        self.spectrogram_dilated_convs = nn.ModuleList()\n",
    "        self.audio_skip_convs = nn.ModuleList()\n",
    "        self.spectrogram_skip_convs = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            dilation = 2 ** i\n",
    "            self.audio_dilated_convs.append(nn.Conv1d(num_channels, num_channels, kernel_size, dilation=dilation, padding=dilation, groups=num_channels//16))\n",
    "            self.spectrogram_dilated_convs.append(nn.Conv1d(num_channels, num_channels, kernel_size, dilation=dilation, padding=dilation, groups=num_channels//16))\n",
    "            self.audio_skip_convs.append(nn.Conv1d(num_channels, num_channels, 1))\n",
    "            self.spectrogram_skip_convs.append(nn.Conv1d(num_channels, num_channels, 1))\n",
    "\n",
    "        # Multi-head attention for combining features\n",
    "        self.feature_attention = nn.MultiheadAttention(embed_dim=num_channels, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "        # Transformer block with residual connection\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=num_channels, nhead=num_heads, dim_feedforward=num_channels * 4, batch_first=True),\n",
    "            num_layers=3)\n",
    "\n",
    "        # Output layers\n",
    "        self.final_conv1 = nn.Conv1d(num_channels, num_channels, 1)\n",
    "        self.final_conv2 = nn.Conv1d(num_channels, audio_channels, 1)\n",
    "\n",
    "        # Additional residual connection across the network\n",
    "        self.residual_conv = nn.Conv1d(num_channels, num_channels, 1)\n",
    "\n",
    "    def forward(self, audio, spectrogram):\n",
    "        # print(\"Original audio shape:\", audio.shape)\n",
    "        # print(\"Original spectrogram shape:\", spectrogram.shape)\n",
    "        \n",
    "        audio_input = F.relu(self.audio_conv(audio))\n",
    "        spectrogram_input = F.relu(self.spectrogram_conv(spectrogram))\n",
    "        audio = audio_input\n",
    "        spectrogram = spectrogram_input\n",
    "        \n",
    "        # print(\"Audio shape:\", audio.shape)\n",
    "        # print(\"Spectrogram shape:\", spectrogram.shape)\n",
    "\n",
    "        audio_skip = 0\n",
    "        spectrogram_skip = 0\n",
    "\n",
    "        # Process through dilated convolutions with residual and skip connections\n",
    "        i = 0\n",
    "        for audio_conv, audio_skip_conv, spectro_conv, spectro_skip_conv in zip(self.audio_dilated_convs, self.audio_skip_convs, self.spectrogram_dilated_convs, self.spectrogram_skip_convs):\n",
    "            # print(\"Audio conv shape:\", audio_conv.shape)\n",
    "            # print(\"Audio skip conv shape:\", audio_skip_conv.shape)\n",
    "            # print(\"Spectrogram conv shape:\", spectro_conv.shape)\n",
    "            # print(\"Spectrogram skip conv shape:\", spectro_skip_conv.shape)\n",
    "            t1 = F.relu(audio_conv(audio))\n",
    "            # print(\"relu shape:\", t1.shape)\n",
    "            # print(\"audio shape:\", audio.shape)\n",
    "            t1 = t1[:, :, : -(2**i)]\n",
    "            # print(\"Modified relu shape:\", t1.shape)\n",
    "            audio = t1 + audio\n",
    "            \n",
    "            t2 = F.relu(spectro_conv(spectrogram))\n",
    "            # print(\"Spectrogram conv shape:\", t2.shape)\n",
    "            # print(\"Spectrogram shape:\", spectrogram.shape)\n",
    "            t2 = t2[:, :, :-(2**i)]\n",
    "            # print(\"Modified Spectrogram conv shape:\", t2.shape)\n",
    "            \n",
    "            spectrogram = t2 + spectrogram\n",
    "            audio_skip += audio_skip_conv(audio)\n",
    "            spectrogram_skip += spectro_skip_conv(spectrogram)\n",
    "            i += 1\n",
    "        \n",
    "        # print(\"Audio skip shape:\", audio_skip.shape)\n",
    "        # print(\"Spectrogram skip shape:\", spectrogram_skip.shape)\n",
    "\n",
    "        # Combine using multi-head attention\n",
    "        combined, _ = self.feature_attention(audio_skip.transpose(1, 2), spectrogram_skip.transpose(1, 2), spectrogram_skip.transpose(1, 2))\n",
    "        combined = combined.transpose(1, 2)\n",
    "        \n",
    "        # print(\"Combined shape:\", combined.shape)\n",
    "\n",
    "        # Transformer processing with residual connection\n",
    "        combined = self.transformer(combined.transpose(1, 2)).transpose(1, 2) + self.residual_conv(combined)\n",
    "\n",
    "        # print(\"Combined shape Transformer:\", combined.shape)\n",
    "        # Final processing\n",
    "        x = F.relu(self.final_conv1(combined))\n",
    "        x = self.final_conv2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def generate_audio(self, audio, spectrogram, sample_no, device='cpu'):\n",
    "        \"\"\"\n",
    "        Generate audio using the model and save it to a directory.\n",
    "\n",
    "        Args:\n",
    "        audio (torch.Tensor): The input audio tensor.\n",
    "        spectrogram (torch.Tensor): The input spectrogram tensor.\n",
    "        sample_no (int): The sample number to append to the filename.\n",
    "        device (str): The device to perform computation on.\n",
    "        \"\"\"\n",
    "        # Ensure the model is in evaluation mode\n",
    "        self.eval()\n",
    "        # Move inputs to the correct device\n",
    "        audio = audio.to(device)\n",
    "        spectrogram = spectrogram.to(device)\n",
    "        # Generate audio using the forward method\n",
    "        with torch.no_grad():\n",
    "            generated_audio = self.forward(audio, spectrogram)\n",
    "        # Ensure the output directory exists\n",
    "        output_dir = 'gen_music'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        # Save the generated audio to a file\n",
    "        output_path = os.path.join(output_dir, f'ultranwav_{sample_no}.wav')\n",
    "        torch.save(generated_audio, output_path)\n",
    "        print(f\"Generated audio saved to {output_path}\")\n",
    "        # Optionally, return the path or the audio tensor for further use\n",
    "        return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import wandb  # Ensure wandb is imported if you're using it\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, epochs, device):\n",
    "    model.to(device)\n",
    "    print(device)\n",
    "    print(\"Training Begins!\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for i, (audio, spectrogram) in enumerate(train_loader):\n",
    "            audio, spectrogram = audio.to(device), spectrogram.to(device)\n",
    "            \n",
    "            if audio.dim() == 2:\n",
    "                audio = audio.unsqueeze(1)  # Add channel dimension\n",
    "            elif audio.dim() != 3:\n",
    "                raise ValueError(\"Audio input must be 2D or 3D tensor\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(audio, spectrogram)\n",
    "            loss = criterion(output, audio)  # Ensure the criterion is correctly defined for the expected output\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Log loss to wandb\n",
    "            wandb.log({\"train_loss\": loss.item()})\n",
    "            if i % 10 == 0:  # Log every 10 steps\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item()}\")\n",
    "\n",
    "            # Save model checkpoint periodically or based on performance\n",
    "            if i % 100 == 0:  # Save every 100 iterations\n",
    "                checkpoint_path = f'TW_Checkpoint/model_TW_{epoch}_{i}.pt'\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "            # Generate synthetic data and add to train_loader if needed\n",
    "            if i % 50 == 0:  # Generate synthetic data every 50 iterations\n",
    "                with torch.no_grad():\n",
    "                    synthetic_audio = model.generate_audio(audio, spectrogram, i, device)\n",
    "                    # Assuming train_loader.dataset is a list or supports append\n",
    "                    train_loader.dataset.append((synthetic_audio.detach(), spectrogram))\n",
    "\n",
    "        epoch_loss /= len(train_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Average Loss: {epoch_loss}\")\n",
    "        wandb.log({\"epoch_loss\": epoch_loss})\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for audio, spectrogram in val_loader:\n",
    "                audio, spectrogram = audio.to(device), spectrogram.to(device)\n",
    "                output = model(audio, spectrogram)\n",
    "                val_loss += criterion(output, audio).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        wandb.log({\"val_loss\": val_loss})\n",
    "        print(f\"Validation Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False, num_workers=0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UltimateTransformerWaveNet().to(device)\n",
    "# print(model)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "# composite_loss = CompositeLoss(perceptual_loss, spectrogram_loss, feature_matching_loss)\n",
    "composite_loss = CompositeLoss(perceptual_loss, spectrogram_loss, feature_matching_loss)\n",
    "\n",
    "# train(model, train_loader, val_loader, optimizer, composite_loss, epochs=50, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Training Begins!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rahat\\miniconda3\\envs\\orc\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, optimizer, composite_loss, epochs=50, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
