{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "from music21 import converter, instrument, note, chord\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_midi_files(midi_folder):\n",
    "    notes = []\n",
    "    for file in os.listdir(midi_folder):\n",
    "        midi = converter.parse(os.path.join(midi_folder, file))\n",
    "        notes_to_parse = None\n",
    "        try:\n",
    "            s2 = instrument.partitionByInstrument(midi)\n",
    "            notes_to_parse = s2.parts[0].recurse() \n",
    "        except:\n",
    "            notes_to_parse = midi.flat.notes\n",
    "\n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                if re.match(r\"[A-G](#|-)?\\d\", str(element.pitch)):\n",
    "                    notes.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                # Filter each note in the chord\n",
    "                chord_notes = '.'.join(str(n) for n in element.normalOrder if re.match(r\"[A-G](#|-)?\\d\", str(n)))\n",
    "                if chord_notes:\n",
    "                    notes.append(chord_notes)\n",
    "    return notes\n",
    "\n",
    "notes = load_midi_files('indian_classical')\n",
    "# Extract the unique pitches in the dataset\n",
    "pitchnames = sorted(set(item for item in notes))\n",
    "# Create a dictionary to map pitches to integers\n",
    "note_to_int = {note: num for num, note in enumerate(pitchnames)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "print(len(note_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sequences\n",
    "sequence_length = 100\n",
    "network_input = []\n",
    "network_output = []\n",
    "\n",
    "for i in range(len(notes) - sequence_length):\n",
    "    sequence_in = notes[i:i + sequence_length]\n",
    "    sequence_out = notes[i + sequence_length]\n",
    "    network_input.append([note_to_int[char] for char in sequence_in])\n",
    "    network_output.append(note_to_int[sequence_out])\n",
    "\n",
    "network_input = np.reshape(network_input, (len(network_input), sequence_length, 1))\n",
    "network_input = torch.tensor(network_input / float(len(pitchnames)), dtype=torch.float32)\n",
    "network_output = torch.tensor(network_output, dtype=torch.long)\n",
    "\n",
    "# Assuming network_input and network_output are already defined\n",
    "train_input, val_input, train_output, val_output = train_test_split(network_input, network_output, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(train_input, train_output)\n",
    "val_dataset = TensorDataset(val_input, val_output)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedMusicLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(AdvancedMusicLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, num_layers=3, dropout=0.3, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)  # Adjust for bidirectional\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc1(x[:, -1, :])\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = AdvancedMusicLSTM(1, 512, len(pitchnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:rovm04h1) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁▂▃▄▅▅▆▇█</td></tr><tr><td>loss</td><td>█▃▃▃▃▂▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation loss</td><td>3.25655</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>3.28579</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dry-forest-3</strong> at: <a href='https://wandb.ai/rebot/music-generation-2/runs/rovm04h1' target=\"_blank\">https://wandb.ai/rebot/music-generation-2/runs/rovm04h1</a><br/> View project at: <a href='https://wandb.ai/rebot/music-generation-2' target=\"_blank\">https://wandb.ai/rebot/music-generation-2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240509_041901-rovm04h1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:rovm04h1). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\SEM6\\DL\\PROJ\\BEGIN\\wandb\\run-20240509_042223-k1tpknk1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rebot/music-generation-2/runs/k1tpknk1' target=\"_blank\">whole-field-4</a></strong> to <a href='https://wandb.ai/rebot/music-generation-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rebot/music-generation-2' target=\"_blank\">https://wandb.ai/rebot/music-generation-2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rebot/music-generation-2/runs/k1tpknk1' target=\"_blank\">https://wandb.ai/rebot/music-generation-2/runs/k1tpknk1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 132.70593287291067\n",
      "Validation Phase, Loss: 63.07322754398469\n",
      "Epoch 1/200, Training Loss: 132.70593287291067, Validation Loss: 63.07322754398469 - Model Saved\n",
      "Epoch 2/200, Loss: 39.63145472926478\n",
      "Epoch 3/200, Loss: 26.61579434333309\n",
      "Epoch 4/200, Loss: 29.19660988930733\n",
      "Epoch 5/200, Loss: 23.115126479056574\n",
      "Epoch 6/200, Loss: 30.095445656007335\n",
      "Epoch 7/200, Loss: 20.83486185535308\n",
      "Epoch 8/200, Loss: 7.793941011351924\n",
      "Epoch 9/200, Loss: 3.525677923233278\n",
      "Epoch 10/200, Loss: 3.48427782135625\n",
      "Epoch 11/200, Loss: 3.8102242696669792\n",
      "Validation Phase, Loss: 8.403663912127096\n",
      "Epoch 11/200, Training Loss: 3.8102242696669792, Validation Loss: 8.403663912127096 - Model Saved\n",
      "Epoch 12/200, Loss: 3.680686575751151\n",
      "Epoch 13/200, Loss: 3.540700387570166\n",
      "Epoch 14/200, Loss: 3.6210102092835212\n",
      "Epoch 15/200, Loss: 3.3405400283875\n",
      "Epoch 16/200, Loss: 3.3111663672231857\n",
      "Epoch 17/200, Loss: 3.3094706054656737\n",
      "Epoch 18/200, Loss: 3.306395257672956\n",
      "Epoch 19/200, Loss: 3.3036360798343534\n",
      "Epoch 20/200, Loss: 3.3039863513362024\n",
      "Epoch 21/200, Loss: 3.3055627826721437\n",
      "Validation Phase, Loss: 3.281678507404943\n",
      "Epoch 21/200, Training Loss: 3.3055627826721437, Validation Loss: 3.281678507404943 - Model Saved\n",
      "Epoch 22/200, Loss: 3.3030420311035646\n",
      "Epoch 23/200, Loss: 3.2936637382353506\n",
      "Epoch 24/200, Loss: 3.2908826547284282\n",
      "Epoch 25/200, Loss: 3.2946547750503785\n",
      "Epoch 26/200, Loss: 3.29435952824931\n",
      "Epoch 27/200, Loss: 3.294729361611028\n",
      "Epoch 28/200, Loss: 3.2913105795460362\n",
      "Epoch 29/200, Loss: 3.2894944625516094\n",
      "Epoch 30/200, Loss: 3.2899745395106654\n",
      "Epoch 31/200, Loss: 3.2888748261236374\n",
      "Validation Phase, Loss: 3.249515241192233\n",
      "Epoch 31/200, Training Loss: 3.2888748261236374, Validation Loss: 3.249515241192233 - Model Saved\n",
      "Epoch 32/200, Loss: 3.2905873079453745\n",
      "Epoch 33/200, Loss: 3.292401009990323\n",
      "Epoch 34/200, Loss: 3.2938448171461783\n",
      "Epoch 35/200, Loss: 3.2873255745057137\n",
      "Epoch 36/200, Loss: 3.292225791561988\n",
      "Epoch 37/200, Loss: 3.289348054316736\n",
      "Epoch 38/200, Loss: 3.2929955586310355\n",
      "Epoch 39/200, Loss: 3.288953402350026\n",
      "Epoch 40/200, Loss: 3.2881848466011787\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\SEM6\\DL\\PROJ\\BEGIN\\two.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SEM6/DL/PROJ/BEGIN/two.ipynb#X13sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m             torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39mbest_model.pth\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SEM6/DL/PROJ/BEGIN/two.ipynb#X13sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m, Training Loss: \u001b[39m\u001b[39m{\u001b[39;00mtotal_loss\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39m\u001b[39mlen\u001b[39m(dataloader)\u001b[39m}\u001b[39;00m\u001b[39m, Validation Loss: \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39m\u001b[39mlen\u001b[39m(validation_dataloader)\u001b[39m}\u001b[39;00m\u001b[39m - Model Saved\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/SEM6/DL/PROJ/BEGIN/two.ipynb#X13sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m train_model_with_checkpoint(model, train_dataloader, validation_dataloader, \u001b[39m200\u001b[39;49m)\n",
      "\u001b[1;32md:\\SEM6\\DL\\PROJ\\BEGIN\\two.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SEM6/DL/PROJ/BEGIN/two.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SEM6/DL/PROJ/BEGIN/two.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/SEM6/DL/PROJ/BEGIN/two.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SEM6/DL/PROJ/BEGIN/two.ipynb#X13sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m avg_loss \u001b[39m=\u001b[39m total_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_dataloader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SEM6/DL/PROJ/BEGIN/two.ipynb#X13sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mavg_loss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "def train_model_with_checkpoint(model, dataloader, validation_dataloader, epochs):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)  # Adjust learning rate\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    wandb.init(project=\"music-generation-2\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, labels in train_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss}')\n",
    "        wandb.log({\"epoch\": epoch + 1, \"loss\": avg_loss})\n",
    "\n",
    "        # Validation phase\n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in validation_dataloader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "            avg_val_loss = val_loss / len(validation_dataloader)\n",
    "            print(f'Validation Phase, Loss: {avg_val_loss}')\n",
    "            wandb.log({\"Validation loss\" : avg_val_loss})\n",
    "\n",
    "        # Scheduler step\n",
    "        scheduler.step()\n",
    "\n",
    "        # Checkpoint model\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Training Loss: {total_loss / len(dataloader)}, Validation Loss: {val_loss / len(validation_dataloader)} - Model Saved')\n",
    "\n",
    "train_model_with_checkpoint(model, train_dataloader, validation_dataloader, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for AdvancedMusicLSTM:\n\tMissing key(s) in state_dict: \"lstm.weight_ih_l0\", \"lstm.weight_hh_l0\", \"lstm.bias_ih_l0\", \"lstm.bias_hh_l0\", \"lstm.weight_ih_l0_reverse\", \"lstm.weight_hh_l0_reverse\", \"lstm.bias_ih_l0_reverse\", \"lstm.bias_hh_l0_reverse\", \"lstm.weight_ih_l1\", \"lstm.weight_hh_l1\", \"lstm.bias_ih_l1\", \"lstm.bias_hh_l1\", \"lstm.weight_ih_l1_reverse\", \"lstm.weight_hh_l1_reverse\", \"lstm.bias_ih_l1_reverse\", \"lstm.bias_hh_l1_reverse\", \"lstm.weight_ih_l2\", \"lstm.weight_hh_l2\", \"lstm.bias_ih_l2\", \"lstm.bias_hh_l2\", \"lstm.weight_ih_l2_reverse\", \"lstm.weight_hh_l2_reverse\", \"lstm.bias_ih_l2_reverse\", \"lstm.bias_hh_l2_reverse\". \n\tUnexpected key(s) in state_dict: \"lstm1.weight_ih_l0\", \"lstm1.weight_hh_l0\", \"lstm1.bias_ih_l0\", \"lstm1.bias_hh_l0\", \"lstm1.weight_ih_l1\", \"lstm1.weight_hh_l1\", \"lstm1.bias_ih_l1\", \"lstm1.bias_hh_l1\", \"batch_norm1.weight\", \"batch_norm1.bias\", \"batch_norm1.running_mean\", \"batch_norm1.running_var\", \"batch_norm1.num_batches_tracked\", \"lstm2.weight_ih_l0\", \"lstm2.weight_hh_l0\", \"lstm2.bias_ih_l0\", \"lstm2.bias_hh_l0\", \"lstm2.weight_ih_l1\", \"lstm2.weight_hh_l1\", \"lstm2.bias_ih_l1\", \"lstm2.bias_hh_l1\", \"batch_norm2.weight\", \"batch_norm2.bias\", \"batch_norm2.running_mean\", \"batch_norm2.running_var\", \"batch_norm2.num_batches_tracked\". \n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([512, 1024]).\n\tsize mismatch for fc1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([54, 256]) from checkpoint, the shape in current model is torch.Size([54, 512]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\SEM6\\DL\\PROJ\\BEGIN\\two.ipynb Cell 7\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SEM6/DL/PROJ/BEGIN/two.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Load the model state\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SEM6/DL/PROJ/BEGIN/two.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbest_model.pth\u001b[39m\u001b[39m'\u001b[39m  \u001b[39m# Replace with your actual model path\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/SEM6/DL/PROJ/BEGIN/two.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(model_path))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SEM6/DL/PROJ/BEGIN/two.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SEM6/DL/PROJ/BEGIN/two.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_music\u001b[39m(model, network_input, pitchnames, note_to_int, num_generate\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\orc\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2184\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[0;32m   2185\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   2186\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2189\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   2190\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2191\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for AdvancedMusicLSTM:\n\tMissing key(s) in state_dict: \"lstm.weight_ih_l0\", \"lstm.weight_hh_l0\", \"lstm.bias_ih_l0\", \"lstm.bias_hh_l0\", \"lstm.weight_ih_l0_reverse\", \"lstm.weight_hh_l0_reverse\", \"lstm.bias_ih_l0_reverse\", \"lstm.bias_hh_l0_reverse\", \"lstm.weight_ih_l1\", \"lstm.weight_hh_l1\", \"lstm.bias_ih_l1\", \"lstm.bias_hh_l1\", \"lstm.weight_ih_l1_reverse\", \"lstm.weight_hh_l1_reverse\", \"lstm.bias_ih_l1_reverse\", \"lstm.bias_hh_l1_reverse\", \"lstm.weight_ih_l2\", \"lstm.weight_hh_l2\", \"lstm.bias_ih_l2\", \"lstm.bias_hh_l2\", \"lstm.weight_ih_l2_reverse\", \"lstm.weight_hh_l2_reverse\", \"lstm.bias_ih_l2_reverse\", \"lstm.bias_hh_l2_reverse\". \n\tUnexpected key(s) in state_dict: \"lstm1.weight_ih_l0\", \"lstm1.weight_hh_l0\", \"lstm1.bias_ih_l0\", \"lstm1.bias_hh_l0\", \"lstm1.weight_ih_l1\", \"lstm1.weight_hh_l1\", \"lstm1.bias_ih_l1\", \"lstm1.bias_hh_l1\", \"batch_norm1.weight\", \"batch_norm1.bias\", \"batch_norm1.running_mean\", \"batch_norm1.running_var\", \"batch_norm1.num_batches_tracked\", \"lstm2.weight_ih_l0\", \"lstm2.weight_hh_l0\", \"lstm2.bias_ih_l0\", \"lstm2.bias_hh_l0\", \"lstm2.weight_ih_l1\", \"lstm2.weight_hh_l1\", \"lstm2.bias_ih_l1\", \"lstm2.bias_hh_l1\", \"batch_norm2.weight\", \"batch_norm2.bias\", \"batch_norm2.running_mean\", \"batch_norm2.running_var\", \"batch_norm2.num_batches_tracked\". \n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([512, 1024]).\n\tsize mismatch for fc1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([54, 256]) from checkpoint, the shape in current model is torch.Size([54, 512])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'AdvancedMusicLSTM' is the class of your model\n",
    "model = AdvancedMusicLSTM(input_size=1, hidden_size=512, output_size=len(pitchnames))  # Adjust parameters as necessary\n",
    "\n",
    "# Load the model state\n",
    "model_path = 'best_model.pth'  # Replace with your actual model path\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "def generate_music(model, network_input, pitchnames, note_to_int, num_generate=500):\n",
    "    \"\"\" Generate music given a sequence of notes \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Pick a random sequence from the input as a starting point for the generation\n",
    "    start = np.random.randint(0, len(network_input)-1)\n",
    "    int_to_note = {num: note for note, num in note_to_int.items()}\n",
    "    pattern = network_input[start].tolist()\n",
    "    prediction_output = []\n",
    "\n",
    "    # Generate notes\n",
    "    for note_index in range(num_generate):\n",
    "        prediction_input = torch.tensor([pattern], dtype=torch.float32).to(device)\n",
    "        prediction = model(prediction_input)\n",
    "        _, index = torch.max(prediction, 1)\n",
    "        \n",
    "        result = int_to_note[index.item()]\n",
    "        prediction_output.append(result)\n",
    "        \n",
    "        pattern.append(index.item() / float(len(pitchnames)))\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "\n",
    "    return prediction_output\n",
    "\n",
    "# Generate a piece of music\n",
    "generated_notes = generate_music(model, network_input, pitchnames, note_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import stream, note, chord, midi\n",
    "\n",
    "def create_midi(prediction_output, output_path='output.mid'):\n",
    "    \"\"\" Convert the output from the prediction to MIDI file \"\"\"\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # Create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        # Pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # Pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # Increase offset each iteration so that notes do not stack\n",
    "        offset += 0.5\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp=output_path)\n",
    "\n",
    "# Create a MIDI file from the generated notes\n",
    "create_midi(generated_notes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
