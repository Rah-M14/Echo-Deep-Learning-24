{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WaveNetModel(nn.Module):\n",
    "    def __init__(self, layers=10, blocks=3, dilation_channels=32, residual_channels=32, skip_channels=256, classes=256):\n",
    "        super(WaveNetModel, self).__init__()\n",
    "        self.dilation_channels = dilation_channels\n",
    "        self.residual_channels = residual_channels\n",
    "        self.skip_channels = skip_channels\n",
    "        self.classes = classes\n",
    "\n",
    "        self.dilations = [2 ** i for i in range(layers)] * blocks\n",
    "        self.start_conv = nn.Conv1d(1, residual_channels, 1)\n",
    "\n",
    "        self.residual_blocks = nn.ModuleList()\n",
    "        self.skip_convs = nn.ModuleList()\n",
    "        self.filter_convs = nn.ModuleList()\n",
    "        self.gate_convs = nn.ModuleList()\n",
    "\n",
    "        for dilation in self.dilations:\n",
    "            self.filter_convs.append(nn.Conv1d(residual_channels, dilation_channels, 2, dilation=dilation))\n",
    "            self.gate_convs.append(nn.Conv1d(residual_channels, dilation_channels, 2, dilation=dilation))\n",
    "            self.skip_convs.append(nn.Conv1d(dilation_channels, skip_channels, 1))\n",
    "            self.residual_blocks.append(nn.Conv1d(dilation_channels, residual_channels, 1))\n",
    "\n",
    "        self.end_conv_1 = nn.Conv1d(skip_channels, skip_channels, 1)\n",
    "        self.end_conv_2 = nn.Conv1d(skip_channels, classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.start_conv(x)\n",
    "        skip_connections = []\n",
    "\n",
    "        for filter_conv, gate_conv, skip_conv, residual_block in zip(self.filter_convs, self.gate_convs, self.skip_convs, self.residual_blocks):\n",
    "            filtered = filter_conv(x)\n",
    "            gated = gate_conv(x)\n",
    "            x_input = F.tanh(filtered) * F.sigmoid(gated)\n",
    "            skip = skip_conv(x_input)\n",
    "            skip_connections.append(skip)\n",
    "            x = residual_block(x_input) + x\n",
    "\n",
    "        x = F.relu(torch.sum(torch.stack(skip_connections), 0))\n",
    "        x = F.relu(self.end_conv_1(x))\n",
    "        x = self.end_conv_2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = WaveNetModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epochs=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(dataloader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(dataloader.dataset)} ({100. * batch_idx / len(dataloader):.0f}%)]\\tLoss: {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, notes, sequence_length=100):\n",
    "        # Convert notes to integer encoding using note_to_int mapping\n",
    "        self.notes = [note_to_int[note] for note in notes]\n",
    "        self.n_vocab = len(set(self.notes))\n",
    "        self.sequence_length = sequence_length\n",
    "        self.data, self.targets = self.prepare_sequences()\n",
    "\n",
    "    def prepare_sequences(self):\n",
    "        data = []\n",
    "        targets = []\n",
    "        for i in range(0, len(self.notes) - self.sequence_length, 1):\n",
    "            sequence_in = self.notes[i:i + self.sequence_length]\n",
    "            sequence_out = self.notes[i + self.sequence_length]\n",
    "            data.append(sequence_in)\n",
    "            targets.append(sequence_out)\n",
    "        return np.array(data), np.array(targets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.from_numpy(self.data[idx]).type(torch.FloatTensor)\n",
    "        x = F.one_hot(x.to(torch.int64), num_classes=self.n_vocab).float()\n",
    "        x = x.permute(1, 0)  # Reshape to [sequence_length, n_vocab] for Conv1d\n",
    "        y = torch.from_numpy(np.array([self.targets[idx]])).type(torch.LongTensor)\n",
    "        return x, y\n",
    "\n",
    "# Assuming 'notes' is a list of all notes/chords from the MIDI files\n",
    "dataset = MusicDataset(notes)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_music(model, dataset, num_notes=500):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Start with a random sequence from the dataset\n",
    "    start_index = np.random.randint(0, len(dataset.data) - 1)\n",
    "    current_sequence = dataset.data[start_index]\n",
    "    generated_notes = []\n",
    "\n",
    "    for _ in range(num_notes):\n",
    "        x = torch.from_numpy(current_sequence).type(torch.FloatTensor)\n",
    "        x = F.one_hot(x.to(torch.int64), num_classes=dataset.n_vocab).float()\n",
    "        x = x.permute(1, 0).unsqueeze(0).to(device)  # Add batch dimension and permute\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = model(x)\n",
    "            predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "\n",
    "        generated_notes.append(int_to_note[predicted_index])\n",
    "        current_sequence = np.roll(current_sequence, -1)\n",
    "        current_sequence[-1] = predicted_index\n",
    "\n",
    "    return generated_notes\n",
    "\n",
    "# Generate a piece of music\n",
    "generated_music = generate_music(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AdvancedMusicLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(AdvancedMusicLSTM, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True, num_layers=2, dropout=0.3)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True, num_layers=2, dropout=0.3)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(hidden_size // 2, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.batch_norm1(x[:, -1, :])  # Apply batch normalization to the output of the last time step\n",
    "        x, _ = self.lstm2(x.unsqueeze(1))\n",
    "        x = self.batch_norm2(x[:, -1, :])\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "model = AdvancedMusicLSTM(1, 512, len(pitchnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rahat\\miniconda3\\envs\\orc\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:bgljpyc9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>loss</td><td>3.01493</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">brisk-totem-3</strong> at: <a href='https://wandb.ai/rebot/music-generation-3/runs/bgljpyc9' target=\"_blank\">https://wandb.ai/rebot/music-generation-3/runs/bgljpyc9</a><br/> View project at: <a href='https://wandb.ai/rebot/music-generation-3' target=\"_blank\">https://wandb.ai/rebot/music-generation-3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240509_043038-bgljpyc9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:bgljpyc9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\SEM6\\DL\\PROJ\\BEGIN\\wandb\\run-20240509_043223-h7z02akz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rebot/music-generation-3/runs/h7z02akz' target=\"_blank\">deep-grass-4</a></strong> to <a href='https://wandb.ai/rebot/music-generation-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rebot/music-generation-3' target=\"_blank\">https://wandb.ai/rebot/music-generation-3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rebot/music-generation-3/runs/h7z02akz' target=\"_blank\">https://wandb.ai/rebot/music-generation-3/runs/h7z02akz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 2.991222266227968\n",
      "Validation Phase, Loss: 3.2523501457706576\n",
      "Best model saved with validation loss: 3.2523501457706576\n",
      "Epoch 2/200, Loss: 2.9472225096917923\n",
      "Epoch 3/200, Loss: 2.910635079106977\n",
      "Epoch 4/200, Loss: 2.8864362605156435\n",
      "Epoch 5/200, Loss: 2.860309354720577\n",
      "Epoch 6/200, Loss: 2.8316703977123385\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "def train_model_with_checkpoint(model, dataloader, validation_dataloader, epochs):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "    criterion = nn.NLLLoss()  # Using NLLLoss which is suitable for LogSoftmax\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    wandb.init(project=\"music-generation-3\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_train_loss = total_loss / len(dataloader)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_train_loss}')\n",
    "        wandb.log({\"epoch\": epoch + 1, \"loss\": avg_train_loss})\n",
    "\n",
    "        # Validation phase\n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in validation_dataloader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "            avg_val_loss = val_loss / len(validation_dataloader)\n",
    "            print(f'Validation Phase, Loss: {avg_val_loss}')\n",
    "            wandb.log({\"Validation loss\" : avg_val_loss})\n",
    "\n",
    "        # Scheduler step\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Checkpoint model\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f'Best model saved with validation loss: {best_loss}')\n",
    "\n",
    "train_model_with_checkpoint(model, train_dataloader, validation_dataloader, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_music(model, network_input, pitchnames, note_to_int, num_generate=500):\n",
    "    \"\"\" Generate music given a sequence of notes \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    \n",
    "    # Pick a random sequence from the input as a starting point for the generation\n",
    "    start = np.random.randint(0, len(network_input)-1)\n",
    "    int_to_note = {num: note for note, num in note_to_int.items()}\n",
    "    pattern = network_input[start].tolist()\n",
    "    prediction_output = []\n",
    "\n",
    "    # Generate notes\n",
    "    for note_index in range(num_generate):\n",
    "        prediction_input = torch.tensor([pattern], dtype=torch.float32).to(device)\n",
    "        prediction = model(prediction_input)\n",
    "        _, index = torch.max(prediction, 1)\n",
    "        \n",
    "        result = int_to_note[index.item()]\n",
    "        prediction_output.append(result)\n",
    "        \n",
    "        pattern.append(index.item() / float(len(pitchnames)))\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "\n",
    "    return prediction_output\n",
    "\n",
    "# Generate a piece of music\n",
    "generated_notes = generate_music(model, network_input, pitchnames, note_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import stream, note, chord, midi\n",
    "\n",
    "def create_midi(prediction_output, output_path='output.mid'):\n",
    "    \"\"\" Convert the output from the prediction to MIDI file \"\"\"\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # Create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        # Pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # Pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # Increase offset each iteration so that notes do not stack\n",
    "        offset += 0.5\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp=output_path)\n",
    "\n",
    "# Create a MIDI file from the generated notes\n",
    "create_midi(generated_notes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
