{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import wandb\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# wandb.init(project='TransformerWaveNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(audio_path, sample_rate=22050, duration=5):\n",
    "    # Load audio file with librosa, automatically resampling to the given sample rate\n",
    "    audio, sr = librosa.load(audio_path, sr=sample_rate, duration=duration)\n",
    "    \n",
    "    # Calculate target number of samples\n",
    "    target_length = sample_rate * duration\n",
    "    \n",
    "    # Pad audio if it is shorter than the target length\n",
    "    if len(audio) < target_length:\n",
    "        padding = target_length - len(audio)\n",
    "        audio = np.pad(audio, (0, padding), mode='constant')\n",
    "    # Truncate audio if it is longer than the target length\n",
    "    elif len(audio) > target_length:\n",
    "        audio = audio[:target_length]\n",
    "    \n",
    "    return audio\n",
    "\n",
    "def get_spectrogram(audio, n_fft=2048, hop_length=512, max_length=130):\n",
    "    # Generate a spectrogram\n",
    "    spectrogram = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)\n",
    "    # Convert to magnitude (amplitude)\n",
    "    spectrogram = np.abs(spectrogram)\n",
    "    \n",
    "    # Pad or truncate the spectrogram to ensure all are the same length\n",
    "    if spectrogram.shape[1] < max_length:\n",
    "        padding = max_length - spectrogram.shape[1]\n",
    "        spectrogram = np.pad(spectrogram, ((0, 0), (0, padding)), mode='constant')\n",
    "    else:\n",
    "        spectrogram = spectrogram[:, :max_length]\n",
    "    \n",
    "    return spectrogram\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, root_dir, sample_rate=22050, n_fft=2048, hop_length=512, max_length=130):\n",
    "        self.root_dir = root_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.max_length = max_length\n",
    "        self.files = [os.path.join(dp, f) for dp, dn, filenames in os.walk(root_dir) for f in filenames if f.endswith('.mp3') or f.endswith('.wav')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.files[idx]\n",
    "        audio = load_audio(audio_path, self.sample_rate)\n",
    "        spectrogram = get_spectrogram(audio, self.n_fft, self.hop_length, self.max_length)\n",
    "        return audio, spectrogram\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "\n",
    "    dataset = AudioDataset(root_dir='DATA')\n",
    "    loader = DataLoader(dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(total_size * train_ratio)\n",
    "    val_size = int(total_size * val_ratio)\n",
    "    test_size = total_size - train_size - val_size  # Ensure all data is used\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "data_folder_path = 'DATA'\n",
    "# dataset = AudioDataset(root_dir=data_folder_path)\n",
    "\n",
    "# Assuming 'dataset' is an instance of AudioDataset\n",
    "train_dataset, val_dataset, test_dataset = split_dataset(dataset)\n",
    "\n",
    "# Create DataLoaders for each dataset split\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in dataset: 407\n",
      "Single batch loaded successfully: [tensor([[ 1.1482e-02,  2.9690e-02,  3.3771e-02,  ..., -5.3938e-03,\n",
      "         -3.2458e-04,  5.0673e-02],\n",
      "        [-3.3641e-02, -6.0243e-02, -5.2029e-02,  ...,  1.2704e-02,\n",
      "          1.7610e-02,  9.4385e-03],\n",
      "        [-6.5417e-03, -8.3363e-03, -1.1754e-03,  ..., -6.6309e-02,\n",
      "         -6.9432e-02, -5.7693e-02],\n",
      "        ...,\n",
      "        [-4.2689e-02, -7.9236e-02, -8.8070e-02,  ..., -2.9538e-01,\n",
      "         -2.9291e-01, -3.2033e-01],\n",
      "        [-2.9800e-03, -1.3442e-02, -7.7015e-03,  ..., -7.4696e-02,\n",
      "         -8.0101e-02, -7.1932e-02],\n",
      "        [-1.3502e-03, -5.8114e-05,  4.2739e-04,  ...,  1.0737e-01,\n",
      "          9.4365e-02,  9.3138e-02]]), tensor([[[6.7074e-02, 4.5210e-03, 2.2363e-02,  ..., 3.4480e-02,\n",
      "          1.6141e-02, 2.5023e-01],\n",
      "         [1.1363e-01, 1.1417e-01, 1.4535e-01,  ..., 9.4468e-02,\n",
      "          1.3799e-01, 1.4693e-01],\n",
      "         [1.5949e-01, 1.8849e-01, 1.8715e-01,  ..., 1.2030e-01,\n",
      "          8.3797e-02, 1.1366e-01],\n",
      "         ...,\n",
      "         [3.5645e-03, 1.7788e-03, 6.2812e-07,  ..., 8.4761e-07,\n",
      "          1.3057e-06, 2.7113e-06],\n",
      "         [3.5622e-03, 1.7778e-03, 2.2519e-07,  ..., 8.7241e-07,\n",
      "          6.2995e-07, 3.4146e-06],\n",
      "         [3.5616e-03, 1.7776e-03, 2.2394e-07,  ..., 8.5392e-07,\n",
      "          7.5385e-07, 2.6902e-06]],\n",
      "\n",
      "        [[4.3138e+00, 2.0016e+00, 5.1649e+00,  ..., 1.1611e+00,\n",
      "          8.7160e-01, 1.0254e+00],\n",
      "         [5.7305e+00, 6.8872e+00, 5.4755e+00,  ..., 3.3493e-01,\n",
      "          8.4129e-01, 1.9254e+00],\n",
      "         [6.1514e+00, 5.6665e+00, 2.5212e+00,  ..., 7.2677e-01,\n",
      "          4.3947e-01, 1.3349e+00],\n",
      "         ...,\n",
      "         [3.8888e-03, 1.9804e-03, 8.0434e-07,  ..., 3.4801e-07,\n",
      "          2.6476e-06, 2.0473e-06],\n",
      "         [3.8827e-03, 1.9775e-03, 4.6458e-07,  ..., 2.0262e-07,\n",
      "          1.8408e-06, 8.0264e-07],\n",
      "         [3.8801e-03, 1.9763e-03, 9.8197e-07,  ..., 2.4603e-07,\n",
      "          1.4704e-06, 2.8594e-08]],\n",
      "\n",
      "        [[1.2223e+00, 4.2537e-01, 3.0596e+00,  ..., 2.5912e+00,\n",
      "          4.8201e-01, 1.8206e+00],\n",
      "         [1.4429e+00, 2.3967e+00, 3.4112e+00,  ..., 3.3754e+00,\n",
      "          6.6637e+00, 8.1584e+00],\n",
      "         [2.3467e+00, 1.8422e+00, 1.8950e+00,  ..., 1.8348e+01,\n",
      "          2.7312e+01, 2.5908e+01],\n",
      "         ...,\n",
      "         [1.2249e-03, 6.2390e-04, 4.0048e-07,  ..., 5.1868e-06,\n",
      "          2.4211e-06, 7.7742e-07],\n",
      "         [1.2275e-03, 6.2222e-04, 2.0719e-06,  ..., 5.7521e-06,\n",
      "          1.8386e-06, 1.3614e-06],\n",
      "         [1.2212e-03, 6.1936e-04, 1.2369e-07,  ..., 4.6212e-06,\n",
      "          2.3311e-06, 1.3464e-06]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[3.9153e+00, 1.9416e+00, 1.8822e-01,  ..., 9.7238e-01,\n",
      "          9.4255e-01, 1.2770e+00],\n",
      "         [3.9356e+00, 1.8409e+00, 1.0111e-01,  ..., 4.5349e-01,\n",
      "          4.4616e-01, 6.9039e-01],\n",
      "         [3.9710e+00, 1.9285e+00, 5.7617e-02,  ..., 2.4146e-01,\n",
      "          3.2229e-01, 6.7107e-01],\n",
      "         ...,\n",
      "         [6.4971e-03, 3.2755e-03, 1.0189e-06,  ..., 1.7716e-06,\n",
      "          2.8426e-06, 3.6709e-06],\n",
      "         [6.4900e-03, 3.2729e-03, 9.5039e-07,  ..., 9.9068e-07,\n",
      "          2.3570e-06, 1.7344e-06],\n",
      "         [6.4893e-03, 3.2720e-03, 1.2353e-06,  ..., 1.4548e-06,\n",
      "          2.5543e-06, 7.9192e-07]],\n",
      "\n",
      "        [[2.4233e-01, 6.0474e-02, 3.5351e-02,  ..., 4.6718e-02,\n",
      "          7.6290e-02, 1.6760e-01],\n",
      "         [2.8252e-01, 1.2795e-01, 8.3708e-02,  ..., 1.0123e-01,\n",
      "          1.0341e-01, 1.0009e-01],\n",
      "         [4.2410e-01, 2.2401e-01, 1.4183e-01,  ..., 4.1863e-02,\n",
      "          6.9352e-02, 4.2193e-02],\n",
      "         ...,\n",
      "         [1.5324e-02, 7.5973e-03, 2.9310e-06,  ..., 2.0518e-07,\n",
      "          8.3960e-07, 1.7481e-06],\n",
      "         [1.5309e-02, 7.5896e-03, 2.0463e-06,  ..., 6.3070e-07,\n",
      "          7.9433e-07, 1.7145e-06],\n",
      "         [1.5308e-02, 7.5911e-03, 7.9053e-07,  ..., 4.5400e-07,\n",
      "          9.2090e-07, 1.7162e-06]],\n",
      "\n",
      "        [[1.3542e+00, 1.1430e+00, 7.9434e-03,  ..., 9.9632e-02,\n",
      "          2.9970e-02, 5.8010e-02],\n",
      "         [1.2028e+00, 1.1992e+00, 5.6298e-01,  ..., 2.3386e-01,\n",
      "          6.3817e-02, 5.4213e-02],\n",
      "         [5.6812e-01, 1.0082e+00, 1.9591e-01,  ..., 1.2846e-01,\n",
      "          4.3019e-01, 5.5642e-01],\n",
      "         ...,\n",
      "         [1.0979e-04, 4.6950e-05, 1.2248e-07,  ..., 2.0698e-07,\n",
      "          2.4478e-08, 2.1088e-07],\n",
      "         [1.0621e-04, 4.5050e-05, 1.4432e-07,  ..., 1.5732e-07,\n",
      "          1.5680e-07, 1.9030e-07],\n",
      "         [1.0526e-04, 4.4228e-05, 1.7040e-08,  ..., 2.0766e-08,\n",
      "          1.5051e-07, 3.2883e-07]]])]\n"
     ]
    }
   ],
   "source": [
    "# Check if the dataset is correctly set up\n",
    "print(\"Number of samples in dataset:\", len(train_dataset))\n",
    "\n",
    "# Create a DataLoader instance (make sure parameters like batch_size are set correctly)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Try to fetch a single batch to see if it works\n",
    "try:\n",
    "    data = next(iter(train_loader))\n",
    "    print(\"Single batch loaded successfully:\", data)\n",
    "except Exception as e:\n",
    "    print(\"Failed to load a batch:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 407\n",
      "Validation set size: 87\n",
      "Test set size: 88\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerWaveNet(nn.Module):\n",
    "    def __init__(self, audio_channels=1, num_channels=64, kernel_size=2, num_blocks=4, num_layers=10, num_heads=8):\n",
    "        super(TransformerWaveNet, self).__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_layers = num_layers\n",
    "        self.dilated_convs = nn.ModuleList()\n",
    "        self.condition_convs = nn.ModuleList()\n",
    "        self.residual_convs = nn.ModuleList()\n",
    "        self.skip_convs = nn.ModuleList()\n",
    "\n",
    "        # Initial convolution layer for raw audio\n",
    "        self.audio_conv = nn.Conv1d(audio_channels, num_channels, 1)\n",
    "        # self.audio_conv = nn.Conv1d(10, out_channels, kernel_size)\n",
    "\n",
    "        # Initial convolution layer for spectrogram\n",
    "        self.spectrogram_conv = nn.Conv1d(audio_channels, num_channels, 1)\n",
    "\n",
    "        # Transformer block\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=num_channels, nhead=num_heads, dim_feedforward=num_channels * 4, batch_first=True),\n",
    "            num_layers=3)\n",
    "\n",
    "        # Dilated convolutions and condition convolutions\n",
    "        for _ in range(num_blocks):\n",
    "            for i in range(num_layers):\n",
    "                dilation = 2 ** i\n",
    "                self.dilated_convs.append(nn.Conv1d(num_channels, 2 * num_channels, kernel_size, dilation=dilation, padding=dilation))\n",
    "                self.condition_convs.append(nn.Conv1d(num_channels, 2 * num_channels, 1))\n",
    "                self.residual_convs.append(nn.Conv1d(num_channels, num_channels, 1))\n",
    "                self.skip_convs.append(nn.Conv1d(num_channels, num_channels, 1))\n",
    "\n",
    "        # Output layers\n",
    "        self.final_conv1 = nn.Conv1d(num_channels, num_channels, 1)\n",
    "        self.final_conv2 = nn.Conv1d(num_channels, audio_channels, 1)\n",
    "\n",
    "    def forward(self, audio, spectrogram):\n",
    "        # Process audio and spectrogram\n",
    "        audio = self.audio_conv(audio)\n",
    "        spectrogram = self.spectrogram_conv(spectrogram)\n",
    "\n",
    "        # Combine audio and spectrogram\n",
    "        x = audio + spectrogram\n",
    "\n",
    "        # Transformer processing\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        skip_connections = []\n",
    "\n",
    "        for b in range(self.num_blocks):\n",
    "            for l in range(self.num_layers):\n",
    "                # Dilated convolution\n",
    "                dilated = self.dilated_convs[b * self.num_layers + l](x)\n",
    "                # Split for gated activation\n",
    "                filtered, gate = torch.split(dilated, dilated.size(1) // 2, dim=1)\n",
    "                x = torch.tanh(filtered) * torch.sigmoid(gate)\n",
    "                # Residual and skip connections\n",
    "                x = self.residual_convs[b * self.num_layers + l](x)\n",
    "                skip = self.skip_convs[b * self.num_layers + l](x)\n",
    "                skip_connections.append(skip)\n",
    "\n",
    "        # Sum all skip connections\n",
    "        x = torch.sum(torch.stack(skip_connections), dim=0)\n",
    "\n",
    "        # Final convolutions\n",
    "        x = F.relu(self.final_conv1(x))\n",
    "        x = self.final_conv2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def generate(self, audio, spectrogram):\n",
    "        \"\"\"\n",
    "        Generate audio using the model in an autoregressive manner.\n",
    "        Assumes the model is already trained and in eval mode.\n",
    "        \"\"\"\n",
    "        self.eval()  # Ensure the model is in evaluation mode\n",
    "        with torch.no_grad():  # No need to track gradients\n",
    "            # Assuming the inputs are already on the correct device and preprocessed\n",
    "            generated_audio = self.forward(audio, spectrogram)\n",
    "\n",
    "            # Post-processing if necessary (e.g., applying a sigmoid to ensure output is in the correct range)\n",
    "            generated_audio = torch.sigmoid(generated_audio)  # Example post-processing\n",
    "\n",
    "        return generated_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\rahat/.cache\\torch\\hub\\harritaylor_torchvggish_master\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load a pre-trained VGGish model for audio feature extraction\n",
    "vggish = torch.hub.load('harritaylor/torchvggish', 'vggish')\n",
    "\n",
    "# Define the Perceptual Loss using VGGish as the feature extractor\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.feature_extractor.eval()  # Set to evaluation mode\n",
    "\n",
    "    def forward(self, generated_audio, target_audio):\n",
    "        with torch.no_grad():\n",
    "            real_features = self.feature_extractor(target_audio)\n",
    "        generated_features = self.feature_extractor(generated_audio)\n",
    "        loss = F.l1_loss(generated_features, real_features)\n",
    "        return loss\n",
    "\n",
    "perceptual_loss = PerceptualLoss(vggish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleSpectrogramLoss(nn.Module):\n",
    "    def __init__(self, scales=[1024, 2048, 4096]):\n",
    "        super(MultiScaleSpectrogramLoss, self).__init__()\n",
    "        self.scales = scales\n",
    "\n",
    "    def forward(self, generated_audio, target_audio):\n",
    "        loss = 0\n",
    "        for scale in self.scales:\n",
    "            gen_spec = torch.stft(generated_audio, n_fft=scale, return_complex=True)\n",
    "            target_spec = torch.stft(target_audio, n_fft=scale, return_complex=True)\n",
    "            loss += F.l1_loss(gen_spec.abs(), target_spec.abs())\n",
    "        return loss / len(self.scales)\n",
    "\n",
    "spectrogram_loss = MultiScaleSpectrogramLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration, let's assume we have a simple CNN as a discriminator\n",
    "class SimpleAudioDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleAudioDiscriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(16 * 16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "    def intermediate_forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return x\n",
    "\n",
    "discriminator = SimpleAudioDiscriminator()\n",
    "\n",
    "class FeatureMatchingLoss(nn.Module):\n",
    "    def __init__(self, discriminator):\n",
    "        super(FeatureMatchingLoss, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.discriminator.eval()\n",
    "\n",
    "    def forward(self, generated_audio, target_audio):\n",
    "        with torch.no_grad():\n",
    "            real_features = self.discriminator.intermediate_forward(target_audio)\n",
    "        generated_features = self.discriminator.intermediate_forward(generated_audio)\n",
    "        loss = F.l1_loss(generated_features, real_features)\n",
    "        return loss\n",
    "\n",
    "feature_matching_loss = FeatureMatchingLoss(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a composite loss\n",
    "class CompositeLoss(nn.Module):\n",
    "    def __init__(self, perceptual_loss, spectrogram_loss, feature_matching_loss):\n",
    "        super(CompositeLoss, self).__init__()\n",
    "        self.perceptual_loss = perceptual_loss\n",
    "        self.spectrogram_loss = spectrogram_loss\n",
    "        self.feature_matching_loss = feature_matching_loss\n",
    "\n",
    "    def forward(self, generated_audio, target_audio):\n",
    "        loss = (self.perceptual_loss(generated_audio, target_audio) +\n",
    "                self.spectrogram_loss(generated_audio, target_audio) +\n",
    "                self.feature_matching_loss(generated_audio, target_audio))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import wandb  # Ensure wandb is imported if you're using it\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, epochs, device):\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for i, (audio, spectrogram) in enumerate(train_loader):\n",
    "            print(\"Here\")\n",
    "            audio, spectrogram = audio.to(device), spectrogram.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(audio, spectrogram)\n",
    "            loss = criterion(output, audio)  # Adjusted to use a proper loss function\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log loss to wandb\n",
    "            wandb.log({\"train_loss\": loss.item()})\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item()}\")\n",
    "            # Save model checkpoint\n",
    "            torch.save(model.state_dict(), f'TW_Checkpoint/model_TW_{epoch}.pt')\n",
    "\n",
    "            # Generate synthetic data and add to train_loader\n",
    "            if i % 10 == 0:  # Every 10 iterations, generate synthetic data\n",
    "                with torch.no_grad():\n",
    "                    synthetic_audio = model.generate(audio, spectrogram)\n",
    "                train_loader.dataset.append((synthetic_audio, spectrogram))\n",
    "\n",
    "        epoch_loss /= len(train_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss}\")\n",
    "        wandb.log({\"epoch_loss\": epoch_loss})\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for audio, spectrogram in val_loader:\n",
    "                audio, spectrogram = audio.to(device), spectrogram.to(device)\n",
    "                output = model(audio, spectrogram)\n",
    "                val_loss += criterion(output, audio).item()\n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "        # Log validation loss to wandb\n",
    "        wandb.log({\"val_loss\": val_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerWaveNet(\n",
      "  (dilated_convs): ModuleList(\n",
      "    (0): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "    (1): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,))\n",
      "    (2): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(4,), dilation=(4,))\n",
      "    (3): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(8,), dilation=(8,))\n",
      "    (4): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(16,), dilation=(16,))\n",
      "    (5): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(32,), dilation=(32,))\n",
      "    (6): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(64,), dilation=(64,))\n",
      "    (7): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(128,), dilation=(128,))\n",
      "    (8): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(256,), dilation=(256,))\n",
      "    (9): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(512,), dilation=(512,))\n",
      "    (10): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "    (11): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,))\n",
      "    (12): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(4,), dilation=(4,))\n",
      "    (13): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(8,), dilation=(8,))\n",
      "    (14): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(16,), dilation=(16,))\n",
      "    (15): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(32,), dilation=(32,))\n",
      "    (16): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(64,), dilation=(64,))\n",
      "    (17): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(128,), dilation=(128,))\n",
      "    (18): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(256,), dilation=(256,))\n",
      "    (19): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(512,), dilation=(512,))\n",
      "    (20): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "    (21): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,))\n",
      "    (22): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(4,), dilation=(4,))\n",
      "    (23): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(8,), dilation=(8,))\n",
      "    (24): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(16,), dilation=(16,))\n",
      "    (25): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(32,), dilation=(32,))\n",
      "    (26): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(64,), dilation=(64,))\n",
      "    (27): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(128,), dilation=(128,))\n",
      "    (28): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(256,), dilation=(256,))\n",
      "    (29): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(512,), dilation=(512,))\n",
      "    (30): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "    (31): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,))\n",
      "    (32): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(4,), dilation=(4,))\n",
      "    (33): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(8,), dilation=(8,))\n",
      "    (34): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(16,), dilation=(16,))\n",
      "    (35): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(32,), dilation=(32,))\n",
      "    (36): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(64,), dilation=(64,))\n",
      "    (37): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(128,), dilation=(128,))\n",
      "    (38): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(256,), dilation=(256,))\n",
      "    (39): Conv1d(64, 128, kernel_size=(2,), stride=(1,), padding=(512,), dilation=(512,))\n",
      "  )\n",
      "  (condition_convs): ModuleList(\n",
      "    (0-39): 40 x Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (residual_convs): ModuleList(\n",
      "    (0-39): 40 x Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (skip_convs): ModuleList(\n",
      "    (0-39): 40 x Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (audio_conv): Conv1d(1, 64, kernel_size=(1,), stride=(1,))\n",
      "  (spectrogram_conv): Conv1d(1, 64, kernel_size=(1,), stride=(1,))\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_conv1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "  (final_conv2): Conv1d(64, 1, kernel_size=(1,), stride=(1,))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False, num_workers=0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerWaveNet().to(device)\n",
    "print(model)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "# composite_loss = CompositeLoss(perceptual_loss, spectrogram_loss, feature_matching_loss)\n",
    "composite_loss = CompositeLoss(perceptual_loss, spectrogram_loss, feature_matching_loss)\n",
    "\n",
    "# train(model, train_loader, val_loader, optimizer, composite_loss, epochs=50, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'audio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43maudio\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpectrogram shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, spectrogram\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'audio' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 1, 1], expected input[1, 10, 110250] to have 1 channels, but got 10 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomposite_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[67], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, optimizer, criterion, epochs, device)\u001b[0m\n\u001b[0;32m     13\u001b[0m audio, spectrogram \u001b[38;5;241m=\u001b[39m audio\u001b[38;5;241m.\u001b[39mto(device), spectrogram\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 15\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspectrogram\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, audio)  \u001b[38;5;66;03m# Adjusted to use a proper loss function\u001b[39;00m\n\u001b[0;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\orc\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\orc\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[62], line 41\u001b[0m, in \u001b[0;36mTransformerWaveNet.forward\u001b[1;34m(self, audio, spectrogram)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, audio, spectrogram):\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Process audio and spectrogram\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     audio \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     spectrogram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspectrogram_conv(spectrogram)\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Combine audio and spectrogram\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\orc\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\orc\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\orc\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\orc\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 1, 1], expected input[1, 10, 110250] to have 1 channels, but got 10 channels instead"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, optimizer, composite_loss, epochs=50, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
